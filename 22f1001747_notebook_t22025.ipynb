{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 99546,
          "databundleVersionId": 11895149,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "22f1001747-notebook-t22025",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preethujohnson/Machine-Learning-Project---Engage-to-Value/blob/main/22f1001747_notebook_t22025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "F3vExm8HSCrS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "engage_2_value_from_clicks_to_conversions_path = kagglehub.competition_download('engage-2-value-from-clicks-to-conversions')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "pN6XS9NrSCrT"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Project\n",
        "\n",
        "**Engage2Value: From Clicks to Conversions**\n",
        "\n",
        "*Predict purchase value from multi-session digital behavior using ML.*\n",
        "\n"
      ],
      "metadata": {
        "id": "DNmk4uWZSCrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### All about the project\n",
        "\n",
        "In today’s digital age, every scroll, click, and visit to a website leaves behind a trail of data. Businesses spend heavily on online ads and marketing campaigns but not all users end up making a purchase. Some just browse, others drop off halfway, and only a few complete a transaction.\n",
        "\n",
        "This project is about building an intelligent system that can analyze the user behavior across multiple digital touchpoints and predict how much a user might spend.\n",
        "\n",
        "Using machine learning, I aim to learn patterns from users who purchased in the past and use that knowledge to predict the spending behavior for new users, that is, their **purchase value**.\n",
        "\n",
        "\n",
        "### Why is this project important?\n",
        "Accurately predicting purchase value helps businesses to:\n",
        "* Maximize returns on ad spending\n",
        "* Target the right audience more effectively\n",
        "* Allocate resources smarter\n",
        "* Personalize user experience to increase conversions\n",
        "\n",
        "\n",
        "### Plan of the Project\n",
        "\n",
        "I will follow a complete **end-to-end machine learning pipeline**, step by step:\n",
        "\n",
        "1. **Understand the dataset**\n",
        "   - Explore and understand the provided training data set. The shape of the dataset,  the features, its type, quality of the data and so on.\n",
        "  \n",
        "2. **Clean and prepare the data**\n",
        "   - Handle the missing values\n",
        "   - Remobe duplicate values\n",
        "   - Encode all the categorical variables\n",
        "   - Scale and impute numerical values as needed\n",
        "   - Engineer useful features\n",
        "\n",
        "3. **Perform Exploratory Data Analysis (EDA)**\n",
        "   - Visualize key relationships and distributions\n",
        "   - Identify potential drivering factors of purchase behavior\n",
        "\n",
        "4. **Build and evaluate multiple ML models**\n",
        "   - Build different ML models like Linear Regression, Tree-based models, XGBoost, LightGBM, Multi-layer Perceptron, etc.\n",
        "   - Use **R² Score** as the main evaluation metric\n",
        "\n",
        "5. **Compare models and select the best**\n",
        "   - Tune hyperparameters\n",
        "   - Avoid overfitting/underfitting\n",
        "   - Choose the model that performs best on validation data\n",
        "\n",
        "6. **Make final predictions and submit**\n",
        "   - Predict purchase value for unseen users\n",
        "   - Generate submission file for Kaggle\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "The project objectives are:\n",
        "- Build a high-performing regression model\n",
        "- Achieve an R² Score ≥ **0.45** on the Kaggle leaderboard. This score helps us unerstand how well our predicton matches the actual value.\n",
        "- Use **at least 3 different models** as per project requirement\n",
        "- Document the entire pipeline clearly and neatly\n",
        "- Be ready to explain every step during the viva voce\n",
        "\n",
        "\n",
        "### 📘 Outcome\n",
        "This project bridges the gap between theory and application. It transforms machine learning from a concept I’ve learned into a tool I can now use to solve real-world problems.\n",
        "\n",
        "By the end of this project, I will have:\n",
        "- Completed a real-world, business-oriented ML project\n",
        "- Gained hands-on experience with data cleaning, EDA, modeling, and evaluation\n",
        "- Produced a clean, reproducible, and professional Kaggle notebook\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JK24K5xuSCrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps I followed**\n",
        "- Data loading\n",
        "- Data Preprocessing\n",
        "- Exploratory Data Analysis\n",
        "- Data visualization\n",
        "- Statistical analysis\n",
        "- Feature Engineering\n",
        "- Train-Validation Split\n",
        "- Building a baseline model\n",
        "- Hyperparameter Tuning\n",
        "- Comparison of models\n",
        "- Predict target value from test data"
      ],
      "metadata": {
        "id": "Q2Ojqjv9SCrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The libraries allowed to use for this project are the following:\n",
        "\n",
        "- NumPy - Used for numerical operations, especially arrays, matrices, and linear algebra.\n",
        "- Pandas - For data manipulation and analysis using dataframes\n",
        "- Matplotlib - A basic visualization library for plotting graphs like line, bar, scatter, etc.\n",
        "- Scikit-learn - Core ML library used for models like LogisticRegression, RandomForest, preprocessing, metrics, train-test splitting, pipelines.\n",
        "- XGBoost - Advanced gradient boosting library. Faster and more accurate than many models.\n",
        "- Seaborn - Built on top of matplotlib, used for beautiful, statistical visualizations.\n",
        "- Imblearn - For dealing with imbalanced datasets\n",
        "- SciPy - Useful for scientific computing, advanced statistical functions, optimization, interpolation, etc.\n",
        "- Pickle - Python's built-in library for saving/loading ML models or data serialization.\n",
        "- regex - Python’s regular expressions module for pattern matching and text cleaning.\n",
        "- Lightgbm - Gradient boosting model like XGBoost, but faster and more memory efficient for large datasets.\n",
        "- Plotly - Interactive visualizations like 3D plots, dashboards, etc.\n",
        "\n",
        "\n",
        "All these libraries are preinstalled in the Kaggle Notebook. But need to import them to work with them.\n",
        "\n"
      ],
      "metadata": {
        "id": "6gA0yC3gSCrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### R² score and RMSE (Root Mean Squared Error)\n",
        "\n",
        "R² Score measures the proportion of variance in the target that is explained by the model.\n",
        "\n",
        "Why it is been used is that it gives a scale-independent evaluation (range: 0 to 1) of how well the model fits the data. It helps quickly understand how much of the variation in purchaseValue is captured by my features.\n",
        "It complements RMSE well. while RMSE shows the average error, R² shows the goodness of fit.RMSE is a key regression metric that measures the average magnitude of prediction error. It penalizes larger errors more than smaller ones, making it useful when we want to be cautious about large deviations. Lower RMSE values indicate better model performance.\n",
        "\n",
        "In this project, I used RMSE along with R² to evaluate and compare all regression models.\n"
      ],
      "metadata": {
        "id": "t9ZNOlNASCrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My End-to-End ML Project Routine\n",
        "1. Data Cleaning\n",
        "- Check duplicates - Remove duplicate rows to avoid bias or redundancy.\n",
        "- Check for variance - Drop constant or near-constant columns that has no predictive power.\n",
        "- Check missing values - If >95% missing, drop the column as imputation will not be reliable. Else, decide imputation strategy (mean, median, mode, etc.).\n",
        "- Check for outliers - Handle skewness (log/sqrt transforms) to make distributions more symmetric.\n",
        "  \n",
        "2. Data Preprocessing\n",
        "- Imputing - fill missing values\n",
        "Numeric → Mean/Median\n",
        "Categorical → Mode/Most frequent\n",
        "\n",
        "- Encoding (for categorical features)\n",
        "One-hot encoding or label encoding as per need.\n",
        "\n",
        "- Scaling (normalize/standardize numeric features)\n",
        "StandardScaler or MinMaxScaler for models sensitive to feature scale (SVM, KNN, etc.).\n",
        "\n",
        "\n",
        "3. Feature Engineering\n",
        "Create new features that may improve predictive power. Example: UNIX Time Stamp\n",
        "\n",
        "4. Model Training & Selection\n",
        "- Train multiple baseline models (Linear, Tree-based, Boosting, etc.).\n",
        "- Compare using validation metrics (R², RMSE for regression; F1, precision, recall for classification).\n",
        "- Select Top 3 best-performing models.\n",
        "  \n",
        "5. Hyperparameter Tuning\n",
        "- Use GridSearchCV / RandomizedSearchCV on the top 3 models.\n",
        "- Optimize parameters like n_estimators, max_depth (trees), learning_rate (boosting), C, gamma (SVM)\n",
        "- Re-compare tuned models.\n",
        "-\n",
        "6. Final Model Selection\n",
        "- Pick the best model based on tuned results.\n",
        "- Train on full training data (train + validation).\n",
        "- Apply to test data for final predictions.\n",
        "  \n",
        "7. Evaluation & Interpretation\n",
        "- Check performance on test set.\n",
        "- Analyze feature importance (if applicable).\n",
        "- Document all steps clearly for reproducibility.\n"
      ],
      "metadata": {
        "id": "lzMRTTyWSCrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me first import all the libraries required and allowed for this project."
      ],
      "metadata": {
        "id": "kmLaKEE_SCrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy import stats\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:16:59.084862Z",
          "iopub.execute_input": "2025-08-08T11:16:59.085211Z",
          "iopub.status.idle": "2025-08-08T11:17:02.412172Z",
          "shell.execute_reply.started": "2025-08-08T11:16:59.085187Z",
          "shell.execute_reply": "2025-08-08T11:17:02.410708Z"
        },
        "id": "tohf3zw3SCrX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "\n",
        "Once the libraries are imported, let me load my training dataset and view how it is like. My train dataset is named as pjtrn."
      ],
      "metadata": {
        "id": "g6V5P92MSCrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:17:34.415402Z",
          "iopub.execute_input": "2025-08-08T11:17:34.416171Z",
          "iopub.status.idle": "2025-08-08T11:17:36.466321Z",
          "shell.execute_reply.started": "2025-08-08T11:17:34.416139Z",
          "shell.execute_reply": "2025-08-08T11:17:36.465261Z"
        },
        "id": "woP36vD1SCrX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Know the features of the data set\n",
        "\n",
        "Now let me get to know more about the dataset. The shape, features, type and all of the data before going with the preprocessing of data."
      ],
      "metadata": {
        "id": "UBlviwy_SCrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of training data:\")\n",
        "pjtrn.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "C-UUz5nSSCrX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The first rows of data looks like: \")\n",
        "pjtrn.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "p6YrRMN5SCrY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dKP9akCoSCrY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Know the Data Types of Columns"
      ],
      "metadata": {
        "id": "DtXesP-pSCrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The data types of the columns are:\")\n",
        "pjtrn.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "id": "Apzjqy9hSCrY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me find out the categorical, numerical and boolean features in the dataset."
      ],
      "metadata": {
        "id": "f0X1GF1LSCrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = pjtrn.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"Categorical Features:\\n\", categorical_features)\n",
        "\n",
        "numerical_features = pjtrn.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "print(\"\\nNumerical Features:\\n\", numerical_features)\n",
        "\n",
        "boolean_features = pjtrn.select_dtypes(include=['bool']).columns.tolist()\n",
        "print(\"\\nBoolean Features:\\n\", boolean_features)"
      ],
      "metadata": {
        "trusted": true,
        "id": "505KIxhUSCrY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks messy, let me arrage the features in table. I am using the Series command for this."
      ],
      "metadata": {
        "id": "q5cjX5fTSCrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_type_df = pd.DataFrame({\n",
        "    'Categorical Features': pd.Series(categorical_features),\n",
        "    'Numerical Features': pd.Series(numerical_features),\n",
        "    'Boolean Features': pd.Series(boolean_features)\n",
        "})\n",
        "feature_type_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "fHL4GZ1rSCrY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I am going to find the descriptive satistics of the data set. This is needed to do the imputation while preprocessing the data."
      ],
      "metadata": {
        "id": "moO20eiHSCrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "0IA0fwExSCrY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me make it a bit more easier to read."
      ],
      "metadata": {
        "id": "DDkNLWstSCrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.describe().T"
      ],
      "metadata": {
        "trusted": true,
        "id": "K4b9P-3bSCrZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With describe() I got the count of values for each column, mean, standard deviation, minimum and maximum value in the column, and the quartile values. The median is missing in this, median is important when we have to impute the numerical values. So, I am finding that."
      ],
      "metadata": {
        "id": "75FHx18cSCrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.median(numeric_only=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "umMfXqqHSCrZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning\n",
        "As I now know the features of my data set, now I am gonna clean it for better quality. I will\n",
        "1. Check for duplicate rows and if any, will drop those rows to avoid redundancy.\n",
        "2. Check for variance, if no variation (constant values), will drop those columns as they add no predictive power.\n",
        "3. Check for missing values, if more than 95% missing values, will drop those columns as they cannot be reliably imputed.\n",
        "4. Check for outliers, transform them if skewed more."
      ],
      "metadata": {
        "id": "IiO1zzCRSCrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let me check for duplicates and drop them."
      ],
      "metadata": {
        "id": "e9mcWFWKSCrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:17:45.026547Z",
          "iopub.execute_input": "2025-08-08T11:17:45.027085Z",
          "iopub.status.idle": "2025-08-08T11:17:45.035576Z",
          "shell.execute_reply.started": "2025-08-08T11:17:45.027047Z",
          "shell.execute_reply": "2025-08-08T11:17:45.034295Z"
        },
        "id": "wO9l-J45SCrZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates_count = pjtrn.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates_count}\")\n",
        "\n",
        "if duplicates_count > 0:\n",
        "    pjtrn.drop_duplicates(inplace=True)\n",
        "    print(\"Duplicates removed.\")\n",
        "else:\n",
        "    print(\"No duplicates found.\")\n",
        "print(\"Shape of new dataset after removing duplicates: \")\n",
        "pjtrn.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:17:46.985364Z",
          "iopub.execute_input": "2025-08-08T11:17:46.986273Z",
          "iopub.status.idle": "2025-08-08T11:17:47.87588Z",
          "shell.execute_reply.started": "2025-08-08T11:17:46.98623Z",
          "shell.execute_reply": "2025-08-08T11:17:47.875089Z"
        },
        "id": "ckggnzyESCrZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There were 236 duplicate rows which cause redundancy, so I removed them."
      ],
      "metadata": {
        "id": "55HnrIvmSCrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let me check for variance."
      ],
      "metadata": {
        "id": "OwCkSuA8SCrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_variance_cols = [col for col in pjtrn.columns if pjtrn[col].nunique() <= 1]\n",
        "\n",
        "\n",
        "print(f\"Number of no-variance columns: {len(no_variance_cols)}\")\n",
        "print(\"No-variance columns:\")\n",
        "\n",
        "for col in no_variance_cols:\n",
        "    print(col)\n",
        "\n",
        "pjtrn[no_variance_cols].head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:17:50.398115Z",
          "iopub.execute_input": "2025-08-08T11:17:50.398457Z",
          "iopub.status.idle": "2025-08-08T11:17:50.722168Z",
          "shell.execute_reply.started": "2025-08-08T11:17:50.398428Z",
          "shell.execute_reply": "2025-08-08T11:17:50.720932Z"
        },
        "id": "3k8ueGmRSCrZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 21 columns which have no variance and add no value to the model training. So I am removing them."
      ],
      "metadata": {
        "id": "5xjt88ZASCrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.drop(columns=no_variance_cols, inplace=True)\n",
        "print(\"Dropped no-variance, constant value columns.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:17:55.227447Z",
          "iopub.execute_input": "2025-08-08T11:17:55.227769Z",
          "iopub.status.idle": "2025-08-08T11:17:55.28988Z",
          "shell.execute_reply.started": "2025-08-08T11:17:55.227746Z",
          "shell.execute_reply": "2025-08-08T11:17:55.288729Z"
        },
        "id": "pVWWo-0tSCra"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:17:58.880949Z",
          "iopub.execute_input": "2025-08-08T11:17:58.881261Z",
          "iopub.status.idle": "2025-08-08T11:17:58.888369Z",
          "shell.execute_reply.started": "2025-08-08T11:17:58.881236Z",
          "shell.execute_reply": "2025-08-08T11:17:58.887258Z"
        },
        "id": "d9l1ppKBSCra"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I am checking the columns with missing values. The columns with more than 95% missing percentage can be removed."
      ],
      "metadata": {
        "id": "bYl88ShMSCra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.isnull()\n",
        "print(pjtrn.isnull().sum())\n",
        "pjtrn.isnull().sum().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:18:01.707494Z",
          "iopub.execute_input": "2025-08-08T11:18:01.707823Z",
          "iopub.status.idle": "2025-08-08T11:18:02.065081Z",
          "shell.execute_reply.started": "2025-08-08T11:18:01.707767Z",
          "shell.execute_reply": "2025-08-08T11:18:02.063958Z"
        },
        "id": "_bULIgUFSCrd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "missing_pct = pjtrn.isnull().mean() * 100\n",
        "print(\"Missing value percentages:\\n\", missing_pct.sort_values(ascending=False))\n",
        "\n",
        "high_missing_cols = missing_pct[missing_pct > 95].index\n",
        "print(\"\\n \\n High missing value columns:\\n\", high_missing_cols)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:18:05.363397Z",
          "iopub.execute_input": "2025-08-08T11:18:05.363685Z",
          "iopub.status.idle": "2025-08-08T11:18:05.484025Z",
          "shell.execute_reply.started": "2025-08-08T11:18:05.363664Z",
          "shell.execute_reply": "2025-08-08T11:18:05.48278Z"
        },
        "id": "L4UuNVjoSCrd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 4 columns with more than 95% missing values. I am removing the columns with more than 95% missing values."
      ],
      "metadata": {
        "id": "bsOWvJhmSCre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.drop(columns=high_missing_cols, inplace=True)\n",
        "print(\"Dropped columns with >95% missing values:\", list(high_missing_cols))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:18:09.747101Z",
          "iopub.execute_input": "2025-08-08T11:18:09.747422Z",
          "iopub.status.idle": "2025-08-08T11:18:09.781301Z",
          "shell.execute_reply.started": "2025-08-08T11:18:09.747397Z",
          "shell.execute_reply": "2025-08-08T11:18:09.780455Z"
        },
        "id": "tIhtbrYsSCre"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "4EF27EzASCre"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I am gonna check the columns with outliers."
      ],
      "metadata": {
        "id": "AmRVudpvSCre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Select numeric columns only\n",
        "numeric_cols = pjtrn.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Dictionary to store outlier counts\n",
        "outlier_summary = {}\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = pjtrn[col].quantile(0.25)\n",
        "    Q3 = pjtrn[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers_count = ((pjtrn[col] < lower_bound) | (pjtrn[col] > upper_bound)).sum()\n",
        "\n",
        "    # Store results if there are outliers\n",
        "    if outliers_count > 0:\n",
        "        outlier_summary[col] = outliers_count\n",
        "\n",
        "# Convert to DataFrame for better display\n",
        "outlier_df = pd.DataFrame(list(outlier_summary.items()), columns=['Column', 'Outlier Count'])\n",
        "\n",
        "# Show table sorted by number of outliers\n",
        "outlier_df = outlier_df.sort_values(by='Outlier Count', ascending=False).reset_index(drop=True)\n",
        "print(outlier_df)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:18:13.062709Z",
          "iopub.execute_input": "2025-08-08T11:18:13.063162Z",
          "iopub.status.idle": "2025-08-08T11:18:13.131434Z",
          "shell.execute_reply.started": "2025-08-08T11:18:13.063129Z",
          "shell.execute_reply": "2025-08-08T11:18:13.130453Z"
        },
        "id": "P_g4U6_SSCre"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 5 columns with outliers. Let me see how these values are spread using boxplots."
      ],
      "metadata": {
        "id": "qKnVGyipSCre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Loop through only columns that have outliers\n",
        "for col in outlier_df['Column']:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=pjtrn[col])\n",
        "    plt.title(f\"Outlier Visualization for {col}\")\n",
        "    plt.xlabel(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:18:16.521529Z",
          "iopub.execute_input": "2025-08-08T11:18:16.521889Z",
          "iopub.status.idle": "2025-08-08T11:18:17.221603Z",
          "shell.execute_reply.started": "2025-08-08T11:18:16.521862Z",
          "shell.execute_reply": "2025-08-08T11:18:17.220499Z"
        },
        "id": "I8IJWkQ6SCre"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first 4 graphs showed the outliers cleary. But the 5th graph looked different. So I checked the column values."
      ],
      "metadata": {
        "id": "REHV9dz6SCre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn['gclIdPresent'].head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "prAEUdkbSCre"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn['gclIdPresent']\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "NWstAa4iSCre"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "gclIdPresent is a binary feature, so the boxplot shows a line at 0 and a single point at 1. The outlier here is not a bad data point but simply the other category. Therefore, no outlier treatment is needed for this column.\n",
        "\n",
        "But other four columns are skewed, so I will do a log transformation to that."
      ],
      "metadata": {
        "id": "aKB-1QnGSCre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Identify numeric columns (exclude binary like gclIdPresent)\n",
        "numeric_cols = pjtrn.select_dtypes(include=[np.number]).columns.tolist()\n",
        "binary_cols = [col for col in numeric_cols if pjtrn[col].nunique() <= 2]\n",
        "numeric_cols = [col for col in numeric_cols if col not in binary_cols]\n",
        "\n",
        "# Function to plot before/after transformation\n",
        "def plot_before_after(col, transformed):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
        "\n",
        "    # Before - Histogram\n",
        "    sns.histplot(pjtrn[col], kde=True, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title(f\"{col} - Histogram (Before)\")\n",
        "\n",
        "    # After - Histogram\n",
        "    sns.histplot(transformed, kde=True, ax=axes[0, 1])\n",
        "    axes[0, 1].set_title(f\"{col} - Histogram (After)\")\n",
        "\n",
        "    # Before - Boxplot\n",
        "    sns.boxplot(x=pjtrn[col], ax=axes[1, 0])\n",
        "    axes[1, 0].set_title(f\"{col} - Boxplot (Before)\")\n",
        "\n",
        "    # After - Boxplot\n",
        "    sns.boxplot(x=transformed, ax=axes[1, 1])\n",
        "    axes[1, 1].set_title(f\"{col} - Boxplot (After)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 2. Transformation rules\n",
        "transformed_data = pjtrn.copy()\n",
        "transformation_summary = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    skewness = pjtrn[col].skew()\n",
        "\n",
        "    if abs(skewness) > 1:  # High skew\n",
        "        transformed_data[col] = np.log1p(pjtrn[col])  # log1p handles zero values safely\n",
        "        transformation_summary.append((col, skewness, \"Log Transformation\"))\n",
        "        plot_before_after(col, transformed_data[col])\n",
        "\n",
        "    elif 0.5 < abs(skewness) <= 1:  # Moderate skew\n",
        "        transformed_data[col] = np.sqrt(pjtrn[col])\n",
        "        transformation_summary.append((col, skewness, \"Square Root Transformation\"))\n",
        "        plot_before_after(col, transformed_data[col])\n",
        "\n",
        "    else:\n",
        "        transformation_summary.append((col, skewness, \"No Transformation\"))\n",
        "\n",
        "# 3. Show summary table\n",
        "transformation_df = pd.DataFrame(transformation_summary, columns=[\"Column\", \"Skewness\", \"Transformation Applied\"])\n",
        "print(transformation_df)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:18:24.455364Z",
          "iopub.execute_input": "2025-08-08T11:18:24.455657Z",
          "iopub.status.idle": "2025-08-08T11:18:48.354414Z",
          "shell.execute_reply.started": "2025-08-08T11:18:24.455636Z",
          "shell.execute_reply": "2025-08-08T11:18:48.353484Z"
        },
        "id": "yJG8oQonSCre"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outliers are also handled now.\n",
        "Now, the duplicate rows are removed, no variance columns are removed, high misisng value columns are removed and the outliers are transformed.\n",
        "Now I have the clean data set. Let me see how it is now."
      ],
      "metadata": {
        "id": "YGpAbn23SCrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:18:48.365344Z",
          "iopub.execute_input": "2025-08-08T11:18:48.365787Z",
          "iopub.status.idle": "2025-08-08T11:18:48.386893Z",
          "shell.execute_reply.started": "2025-08-08T11:18:48.36576Z",
          "shell.execute_reply": "2025-08-08T11:18:48.38575Z"
        },
        "id": "R1Ng_CU6SCrf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Clean Dataset\n",
        "\n",
        "Now, I have the clean dataset without duplicate rows, no variance columns, high misisng value columns and outliers. Now, I can go for the preprocessing of data."
      ],
      "metadata": {
        "id": "jbPRiG96SCrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace pjtrn with the transformed data\n",
        "pjtrn = transformed_data.copy()\n",
        "\n",
        "# Confirm the shape is the same but data values are updated\n",
        "print(pjtrn.shape)\n",
        "pjtrn.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:19:04.734128Z",
          "iopub.execute_input": "2025-08-08T11:19:04.734469Z",
          "iopub.status.idle": "2025-08-08T11:19:04.799888Z",
          "shell.execute_reply.started": "2025-08-08T11:19:04.734444Z",
          "shell.execute_reply": "2025-08-08T11:19:04.798875Z"
        },
        "id": "xq8zGTsSSCrf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "Now that we have clean data, we can do some preprocessing to our date to make it more meaningful and useful for modelling.\n",
        "1. Imputation (Handling Missing Values in Remaining Columns)\n",
        "Even after removing high-missing columns, some columns may still have smaller percentages of missing values. I am handling them like this:\n",
        "- Numerical columns → Fill with mean or median\n",
        "- Categorical columns → Fill with mode (most frequent value)\n",
        "\n",
        "2. Encoding (Converting Categorical → Numerical) - Converting categorical (text or label) data into numerical form so machine learning models can process it.\n",
        "Machine learning models (like Random Forest, XGBoost) can’t handle raw text labels. So they have to be encoded to numerical values.\n",
        "- One-Hot Encoding → For nominal categories (no order, e.g., “red”, “blue”)\n",
        "- Label Encoding → For ordinal categories (has order, e.g., “low”, “medium”, “high”)\n",
        "\n",
        "3. Scaling (Standardizing Numerical Values) - Adjusting numerical feature values to a common range or distribution without changing their relationships.\n",
        "Scaling is needed for algorithms sensitive to feature magnitude (SVM, KNN, Logistic Regression), but tree-based models (RandomForest, XGBoost) don’t require it.\n",
        "- Min-Max Scaler - Scales data to a fixed range, usually [0, 1] Preserves shape but compresses values into the range\n",
        "- Standard Scaler (Z-score Normalization) - Centers data to mean 0 and standard deviation 1.\n"
      ],
      "metadata": {
        "id": "_iFPi7qQSCrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing Misisng Values\n",
        "I am now gonna impute the missing values in the columns with low misisng data."
      ],
      "metadata": {
        "id": "3cHaAvIdSCrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = pjtrn.select_dtypes(include=['number']).columns.tolist()\n",
        "categorical_cols = pjtrn.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(\"Numerical columns:\", numerical_cols)\n",
        "print(\"Categorical columns:\", categorical_cols)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:19:11.562715Z",
          "iopub.execute_input": "2025-08-08T11:19:11.563137Z",
          "iopub.status.idle": "2025-08-08T11:19:11.595378Z",
          "shell.execute_reply.started": "2025-08-08T11:19:11.563111Z",
          "shell.execute_reply": "2025-08-08T11:19:11.594141Z"
        },
        "id": "3WwyM_WVSCrf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# I am checking the skewness of columns\n",
        "\n",
        "skewness = pjtrn[numerical_cols].skew().sort_values(ascending=False)\n",
        "print(\"Skewness of numerical columns:\")\n",
        "print(skewness)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:19:14.330616Z",
          "iopub.execute_input": "2025-08-08T11:19:14.330955Z",
          "iopub.status.idle": "2025-08-08T11:19:14.351681Z",
          "shell.execute_reply.started": "2025-08-08T11:19:14.33093Z",
          "shell.execute_reply": "2025-08-08T11:19:14.350551Z"
        },
        "id": "skshpXO0SCrf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical columns skewness-based imputation\n",
        "median_impute_cols = ['gclIdPresent', 'sessionNumber', 'purchaseValue']\n",
        "mean_impute_cols = ['totalHits', 'pageViews', 'sessionStart', 'sessionId', 'date', 'userId']\n",
        "\n",
        "# Impute median for high-skew columns\n",
        "for col in median_impute_cols:\n",
        "    pjtrn[col].fillna(pjtrn[col].median(), inplace=True)\n",
        "\n",
        "# Impute mean for low/moderate skew columns\n",
        "for col in mean_impute_cols:\n",
        "    pjtrn[col].fillna(pjtrn[col].mean(), inplace=True)\n",
        "\n",
        "# Impute mode for categorical columns\n",
        "for col in categorical_cols:\n",
        "    pjtrn[col].fillna(pjtrn[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"Missing values imputed successfully.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:19:17.321923Z",
          "iopub.execute_input": "2025-08-08T11:19:17.322277Z",
          "iopub.status.idle": "2025-08-08T11:19:17.606707Z",
          "shell.execute_reply.started": "2025-08-08T11:19:17.322251Z",
          "shell.execute_reply": "2025-08-08T11:19:17.605647Z"
        },
        "id": "kokZtF0VSCrf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_counts = pjtrn.isnull().sum()\n",
        "print(missing_counts[missing_counts > 0])\n",
        "print(\"Total missing values in dataset:\", pjtrn.isnull().sum().sum())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:19:21.437431Z",
          "iopub.execute_input": "2025-08-08T11:19:21.43833Z",
          "iopub.status.idle": "2025-08-08T11:19:21.6692Z",
          "shell.execute_reply.started": "2025-08-08T11:19:21.438294Z",
          "shell.execute_reply": "2025-08-08T11:19:21.667901Z"
        },
        "id": "jZQ9mL_gSCrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding\n",
        "As all missing values are imputed, next I am gonna encode all the categorical variables.\n",
        "How to decide which encoding to use depends on the type and nature of your categorical variables:\n",
        "1. One-Hot Encoding\n",
        "Use for nominal categorical variables (categories with no order)\n",
        "Example: colors (red, blue, green), product types, city names\n",
        "Creates new binary columns for each category\n",
        "\n",
        "2. Label Encoding\n",
        "Use for ordinal categorical variables (categories with a clear order)\n",
        "Example: low, medium, high; education levels (high school < college < masters)\n",
        "Converts categories to integers (0,1,2,...) preserving order\n",
        "\n",
        "So, first I have to know what type my categorical columns are."
      ],
      "metadata": {
        "id": "3Q_1gQmUSCrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify categorical columns\n",
        "categorical_cols = pjtrn.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(\"Categorical columns found:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "# Step 2: Display unique values for each categorical column to help decide type\n",
        "\"\"\"for col in categorical_cols:\n",
        "    unique_vals = pjtrn[col].unique()\n",
        "    print(f\"\\nColumn: {col}\")\n",
        "    print(f\"Unique values ({len(unique_vals)}): {unique_vals}\")\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:20:23.421961Z",
          "iopub.execute_input": "2025-08-08T11:20:23.422352Z",
          "iopub.status.idle": "2025-08-08T11:20:23.444383Z",
          "shell.execute_reply.started": "2025-08-08T11:20:23.422324Z",
          "shell.execute_reply": "2025-08-08T11:20:23.442891Z"
        },
        "id": "OXWtMkYLSCrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results, I can see that all columns are nominal and nothing is ordinal. So, the best and safest approach is to use One-Hot Encoding for all of them."
      ],
      "metadata": {
        "id": "F01o1BD3SCrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pjtrn = pd.get_dummies(pjtrn, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "#print(\"One-Hot Encoding applied. New shape:\", pjtrn.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7noXMktBSCrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of columns increased dramatically after encoding. Which I dont feel good. So let me try another approach.\n",
        "1. Check cardinality (unique values) of categorical columns before encoding\n",
        "If some columns have very high cardinality (like userId or sessionId), encoding them fully isn’t useful.\n",
        "Drop or exclude high-cardinality ID-like columns (e.g., userId, sessionId) from encoding because they don’t generalize well to unseen data.\n",
        "\n",
        "2. Use alternative encoding for high-cardinality columns:\n",
        "\n",
        "Target encoding (mean target value per category)\n",
        "Frequency encoding (replace category with its frequency)\n",
        "\n",
        "3. Hashing trick (hash categories into fixed number of bins)\n",
        "One-Hot Encode only low-cardinality categorical columns and treat high-cardinality columns differently or drop them if not important."
      ],
      "metadata": {
        "id": "cY5VG50qSCrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_cols:\n",
        "    print(f\"{col}: {pjtrn[col].nunique()} unique values\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:20:51.769747Z",
          "iopub.execute_input": "2025-08-08T11:20:51.770123Z",
          "iopub.status.idle": "2025-08-08T11:20:51.896507Z",
          "shell.execute_reply.started": "2025-08-08T11:20:51.770098Z",
          "shell.execute_reply": "2025-08-08T11:20:51.895408Z"
        },
        "id": "5ZJUCKZhSCrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For low cardinality (< 30 unique) columns, use One-Hot Encoding.\n",
        "\n",
        "For high cardinality (> 30 unique) columns, use Frequency Encoding or Target Encoding. Frequency encoding is a simple way to convert categorical variables into numbers by replacing each category with the frequency (count or proportion) of that category in the dataset.\n",
        "How it works:\n",
        "For each category in a column, count how many times it appears in the dataset.\n",
        "\n",
        "Replace every occurrence of that category with this count (or proportion)."
      ],
      "metadata": {
        "id": "TLDDZZm9SCrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists based on your unique values info\n",
        "one_hot_cols = ['browser', 'geoCluster', 'trafficSource.campaign', 'geoNetwork.networkDomain',\n",
        "                'os', 'geoNetwork.subContinent', 'trafficSource.medium', 'deviceType',\n",
        "                'userChannel', 'geoNetwork.continent']\n",
        "\n",
        "freq_encode_cols = ['trafficSource.keyword', 'geoNetwork.region', 'trafficSource',\n",
        "                    'locationCountry', 'geoNetwork.city', 'geoNetwork.metro',\n",
        "                    'trafficSource.referralPath']\n",
        "\n",
        "# 1. Frequency Encoding\n",
        "for col in freq_encode_cols:\n",
        "    freq = pjtrn[col].value_counts(normalize=True)  # use proportion for scaling\n",
        "    pjtrn[col] = pjtrn[col].map(freq)\n",
        "\n",
        "# 2. One-Hot Encoding with drop_first=True to avoid dummy trap\n",
        "pjtrn = pd.get_dummies(pjtrn, columns=one_hot_cols, drop_first=True)\n",
        "\n",
        "print(\"Encoding done. New dataset shape:\", pjtrn.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:24:23.613562Z",
          "iopub.execute_input": "2025-08-08T11:24:23.61392Z",
          "iopub.status.idle": "2025-08-08T11:24:23.888309Z",
          "shell.execute_reply.started": "2025-08-08T11:24:23.613894Z",
          "shell.execute_reply": "2025-08-08T11:24:23.887317Z"
        },
        "id": "iMj-6nsOSCrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for categorical columns in the dataset\n",
        "categorical_cols_after_encoding = pjtrn.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "if len(categorical_cols_after_encoding) == 0:\n",
        "    print(\"No categorical columns remain in the dataset.\")\n",
        "else:\n",
        "    print(\"Categorical columns found:\", categorical_cols_after_encoding)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:27:05.26482Z",
          "iopub.execute_input": "2025-08-08T11:27:05.26516Z",
          "iopub.status.idle": "2025-08-08T11:27:05.272072Z",
          "shell.execute_reply.started": "2025-08-08T11:27:05.265135Z",
          "shell.execute_reply": "2025-08-08T11:27:05.27074Z"
        },
        "id": "pOgGSz8BSCrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split\n",
        "Now as I have clean data with no missing values and all values encoded, I can move to the tran-test split of data for modelling."
      ],
      "metadata": {
        "id": "D57qcmeRSCrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pjtrn.columns.tolist())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:28:18.289915Z",
          "iopub.execute_input": "2025-08-08T11:28:18.290323Z",
          "iopub.status.idle": "2025-08-08T11:28:18.296445Z",
          "shell.execute_reply.started": "2025-08-08T11:28:18.2903Z",
          "shell.execute_reply": "2025-08-08T11:28:18.295494Z"
        },
        "id": "t1hNJM05SCrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "purchaseValue is my target column, so I am dropping that column and splitting the train and test at 80% and 20%."
      ],
      "metadata": {
        "id": "LJvAUUDDSCrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'target' is your target column name\n",
        "X = pjtrn.drop('purchaseValue', axis=1)  # Features\n",
        "y = pjtrn['purchaseValue']               # Target\n",
        "\n",
        "# Split 80% train, 20% test with random state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T11:28:53.732981Z",
          "iopub.execute_input": "2025-08-08T11:28:53.733368Z",
          "iopub.status.idle": "2025-08-08T11:28:53.804949Z",
          "shell.execute_reply.started": "2025-08-08T11:28:53.733345Z",
          "shell.execute_reply": "2025-08-08T11:28:53.803976Z"
        },
        "id": "XLR42WItSCrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the dataset with ML Models\n",
        "I am using 5 different models.\n",
        "1. Linear Regression\n",
        "Linear Regression is the simplest regression model that assumes a linear relationship between input features and the continuous target variable. It fits a line (or hyperplane) that minimizes the sum of squared differences between actual and predicted values.\n",
        "Why use it:\n",
        "- Acts as a strong baseline to compare other models against.\n",
        "- Easy to interpret coefficients.\n",
        "- Fast to train and requires less computational power.\n",
        "\n",
        "2. Random Forest Regressor\n",
        "Random Forest is an ensemble of decision trees where each tree is trained on a random subset of data and features. The final prediction is the average of predictions from all trees.\n",
        "Why use it:\n",
        "- Handles nonlinear relationships well.\n",
        "- Robust to outliers and noise.\n",
        "- Less prone to overfitting compared to single decision trees.\n",
        "- Requires minimal data preprocessing (no scaling needed).\n",
        "\n",
        "3. Decision Tree Regressor\n",
        "A Decision Tree Regressor splits the data into regions based on feature values by creating a tree of decisions. It predicts the target by averaging the values in each region (leaf node).\n",
        "Why use it:\n",
        "- Easy to understand and interpret (visualizable).\n",
        "- Captures nonlinear relationships.\n",
        "- Fast to train on moderate-sized datasets.\n",
        "- Serves as a good simple nonlinear baseline before ensembles.\n",
        "  \n",
        "4. XGBoost Regressor\n",
        "XGBoost (Extreme Gradient Boosting) is a powerful boosting algorithm that builds trees sequentially, where each new tree attempts to correct errors made by previous trees. It uses gradient descent optimization and regularization.\n",
        "Why use it:\n",
        "- Often achieves state-of-the-art performance on tabular data.\n",
        "- Efficient and fast with built-in handling of missing values.\n",
        "- Supports parallel processing.\n",
        "\n",
        "5. Support Vector Regressor (SVR)\n",
        "SVR extends Support Vector Machines to regression problems by fitting the best line within a margin (epsilon) around the data points. It tries to keep errors within this margin, effectively ignoring small errors.\n",
        "Why use it:\n",
        "- Effective for datasets with smaller sizes.\n",
        "- Can model nonlinear relationships using kernels (e.g., RBF kernel).\n",
        "- Good if you expect sparse data or need robustness to outliers."
      ],
      "metadata": {
        "id": "5xtY4BELSCrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define models with their names\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(random_state=42, verbosity=0),\n",
        "\n",
        "}\n",
        "\n",
        "# Assuming X_train, y_train, X_test, y_test are already defined\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Create pipeline — if you have preprocessing, you can add it here\n",
        "    pipeline = Pipeline([\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # Train\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{name} Performance:\")\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"R-squared: {r2:.4f}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T12:35:43.351554Z",
          "iopub.execute_input": "2025-08-08T12:35:43.351918Z",
          "iopub.status.idle": "2025-08-08T12:37:38.433148Z",
          "shell.execute_reply.started": "2025-08-08T12:35:43.35189Z",
          "shell.execute_reply": "2025-08-08T12:37:38.43241Z"
        },
        "id": "FhMEv8TeSCrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning\n",
        "As I now have the best performing models, I am gonna do the hyperparameter tuning of them to enhance the results.\n",
        "Hyperparameter tuning is the process of systematically searching for the best combination of hyperparameters (settings) for a machine learning model to optimize its performance. Unlike model parameters learned during training (e.g., weights in regression), hyperparameters are set before training and control the learning process.\n",
        "- Different hyperparameter values can significantly impact model accuracy, generalization, and training time.\n",
        "- Tuning helps prevent overfitting or underfitting by finding the right balance.\n",
        "- It enables models to learn the underlying patterns more effectively.\n",
        "\n",
        "Models\tand their Important Hyperparameters\n",
        "- Random Forest - Number of trees (n_estimators), max tree depth (max_depth), min samples per leaf (min_samples_leaf)\n",
        "- XGBoost -\tLearning rate (eta), number of trees (n_estimators), max depth (max_depth), subsample ratio (subsample)\n",
        "- Decision Tree\t- Max depth (max_depth), min samples split (min_samples_split)\n",
        "- Support Vector Regressor (SVR) - Kernel type, regularization parameter (C), epsilon (epsilon)"
      ],
      "metadata": {
        "id": "b2lUMTdRSCri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid (keep small for GridSearchCV)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,              # 3-fold cross-validation\n",
        "    scoring='neg_mean_squared_error',  # Use MSE for scoring\n",
        "    n_jobs=-1,         # Use all cores\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "print(f\"Best RMSE: {(-grid_search.best_score_)**0.5:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T12:39:23.919678Z",
          "iopub.execute_input": "2025-08-08T12:39:23.920033Z",
          "iopub.status.idle": "2025-08-08T13:24:20.570719Z",
          "shell.execute_reply.started": "2025-08-08T12:39:23.920008Z",
          "shell.execute_reply": "2025-08-08T13:24:20.569521Z"
        },
        "id": "v45cBzZwSCri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter distributions (can be wider)\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150, 200, 300],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,         # Number of parameter settings sampled\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(f\"Best RMSE: {(-random_search.best_score_)**0.5:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "LBHwgbngSCri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define pipelines for each model\n",
        "pipelines = {\n",
        "    'RandomForest': Pipeline([('model', RandomForestRegressor(random_state=42))]),\n",
        "    'DecisionTree': Pipeline([('model', DecisionTreeRegressor(random_state=42))]),\n",
        "    'XGBoost': Pipeline([('model', xgb.XGBRegressor(random_state=42, verbosity=0))])\n",
        "}\n",
        "\n",
        "# Hyperparameter distributions for tuning (inside 'model' step)\n",
        "param_distributions = {\n",
        "    'RandomForest': {\n",
        "        'model__n_estimators': [50, 100, 150],\n",
        "        'model__max_depth': [10, 20, None],\n",
        "        'model__min_samples_split': [2, 5],\n",
        "        'model__min_samples_leaf': [1, 2]\n",
        "    },\n",
        "    'DecisionTree': {\n",
        "        'model__max_depth': [None, 5, 10, 20],\n",
        "        'model__min_samples_split': [2, 5, 10],\n",
        "        'model__min_samples_leaf': [1, 2, 4],\n",
        "        'model__max_features': [None, 'auto', 'sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model__n_estimators': [50, 100, 150, 200],\n",
        "        'model__max_depth': [3, 5, 7],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'model__subsample': [0.6, 0.8, 1.0],\n",
        "        'model__colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'model__reg_alpha': [0, 0.1, 0.5],\n",
        "        'model__reg_lambda': [1, 1.5, 2]\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name in ['RandomForest', 'DecisionTree', 'XGBoost']:\n",
        "    print(f\"\\nTuning hyperparameters for {model_name}...\\n\")\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipelines[model_name],\n",
        "        param_distributions=param_distributions[model_name],\n",
        "        n_iter=20,\n",
        "        cv=3,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    best_params = random_search.best_params_\n",
        "    best_rmse = (-random_search.best_score_)**0.5\n",
        "\n",
        "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
        "    print(f\"Best RMSE for {model_name}: {best_rmse:.4f}\")\n",
        "\n",
        "    # Save results for reference\n",
        "    results[model_name] = {'best_params': best_params, 'best_rmse': best_rmse}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "UGmHeSGnSCri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation on Test Data"
      ],
      "metadata": {
        "id": "2TtwWxm0SCri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv\")\n",
        "y_pred_test = best_model.predict(X_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KldWrVYhSCri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submitting the file"
      ],
      "metadata": {
        "id": "S3KQVK8dSCri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'id': test_df['id'],       # Replace 'id' with the actual ID column name in your test set\n",
        "    'purchaseValue': y_pred    # The target predictions\n",
        "})\n",
        "\n",
        "# Save submission to CSV\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission.csv' created successfully.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "7hZ9X8I-SCri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L1"
      ],
      "metadata": {
        "id": "THfAamyVSCri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 4. Outlier handling (numeric columns only)\n",
        "numeric_cols = pjtrn.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = pjtrn[col].quantile(0.25)\n",
        "    Q3 = pjtrn[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers_count = ((pjtrn[col] < lower_bound) | (pjtrn[col] > upper_bound)).sum()\n",
        "    print(outliers_count)\n",
        "    \"\"\"if outliers_count > 0:\n",
        "        print(f\"{col}: {outliers_count} outliers detected — capping values.\")\n",
        "        pjtrn[col] = np.where(pjtrn[col] < lower_bound, lower_bound,\n",
        "                              np.where(pjtrn[col] > upper_bound, upper_bound, pjtrn[col]))\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5Py4ETYQSCrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me have some visulaizations of the data I have."
      ],
      "metadata": {
        "id": "ColKSYDHSCrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hzZyWwYySCrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(pjtrn['purchaseValue'], bins=50, kde=True)\n",
        "plt.title('Distribution of Purchase Value')\n",
        "plt.xlabel('Purchase Value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "1eg3sVCQSCrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows that `purchaseValue` is highly right-skewed with many low values and a few very high outliers.\n",
        "Log transformation is applied to normalize this distribution for better model performance.\n"
      ],
      "metadata": {
        "id": "dcZcE0lJSCrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "pjtrn['purchaseValue_log'] = np.log1p(pjtrn['purchaseValue'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(pjtrn['purchaseValue_log'], bins=50, kde=True)\n",
        "plt.title('Distribution of Log-Transformed Purchase Value')\n",
        "plt.xlabel('Log(Purchase Value)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "llp3A6-GSCrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(pjtrn.isnull(), cbar=False, cmap=\"viridis\")\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "njpNCX99SCrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visual highlights which features have missing values and how widespread they are.\n",
        "It help me decide the right imputation strategies like mean, median, or mode.\n"
      ],
      "metadata": {
        "id": "xIFDsGJgSCrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pjtrn.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "fqvmyuxGSCrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap helps identify strong linear relationships between numerical features.\n",
        "I use this to detect multicollinearity and select features most related to the target."
      ],
      "metadata": {
        "id": "Gqtie6Z_SCrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(data=pjtrn, x='deviceType', y='purchaseValue')\n",
        "plt.title('Purchase Value by Device Type')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "G62UK0N0SCrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots show the spread and presence of outliers in numerical features.\n",
        "They guide me in deciding whether to cap/fix outliers or apply transformations.\n"
      ],
      "metadata": {
        "id": "na9W7RT9SCrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(data=pjtrn, x='browser', order=pjtrn['browser'].value_counts().index)\n",
        "plt.title('Count of Users by Browser')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "lEwkuIgpSCrk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplots display the distribution of categorical feature values.\n",
        "This helps identify dominant categories and whether encoding is needed.\n"
      ],
      "metadata": {
        "id": "2M6Uyq7LSCrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Now, I need to know the missing values, duplicate values and outliers in the dataset. Need to know the columns which have missing values, percentage of missing value and their order. this is important because the training dataset need to be clean for model training. Or else the results will be biased.\n",
        "\n",
        "### Missing Values Analysis\n",
        "Let me start by finding out the missing values in the columns in the data set. I am using the isnull() command for that."
      ],
      "metadata": {
        "id": "wuGwTurpSCrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.isnull()"
      ],
      "metadata": {
        "trusted": true,
        "id": "uGLdwRA1SCrk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.isnull().sum()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ji5nxKiBSCrk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.isnull().sum().sum()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FjlgBH8WSCrk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cols = pjtrn.columns[pjtrn.isnull().any()]\n",
        "pjtrn[missing_cols].T"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ns2wiSn_SCrk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "missing_percent = (pjtrn.isnull().sum() / len(pjtrn)) * 100\n",
        "(pjtrn.isnull().sum() / len(pjtrn)) * 100"
      ],
      "metadata": {
        "trusted": true,
        "id": "W995hnLXSCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Missing Value Analysis Results\n",
        "From this I understood that, 11 columns have missing values, and a total of 882719 values are misisng. The percentage of misisng values are also found out. The rows without misisng values are clean and ready to use. The columns with less than 30% missing values can be imputed for a better usage. And it is wise to drop the columns that has more than 95% missing values as they can barely contribute to the model training.\n",
        "\n",
        "- I observed that several features have a significant proportion of missing values.\n",
        "- Features like `trafficSource.adContent`, `adwordsClickInfo.slot`, and others have more than **95% missing values**, making them less useful for modeling.\n",
        "- Some features like `trafficSource.keyword`, `totals.bounces`, and `new_visits` have moderate missing values and could still carry useful information.\n",
        "\n",
        "So, I am gonna drop the high missing values and impute the rest."
      ],
      "metadata": {
        "id": "p9Qsa9W1SCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of columns in the dataset:\", pjtrn.shape[1])\n",
        "print(\"The highest percent o\")\n",
        "high_missing = missing_percent[missing_percent > 95]\n",
        "print(high_missing)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SvLN3VivSCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on missing percentages and business understanding, I will **drop** features with more than 95% missing values and plan to **impute** or process the remaining ones accordingly.\n",
        "First let me drop the highly missing value columns, those with more than 95% missing values.\n"
      ],
      "metadata": {
        "id": "XE46rx8cSCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.drop(columns=high_missing.index, inplace=True, errors='ignore')"
      ],
      "metadata": {
        "trusted": true,
        "id": "afl4BD0PSCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I dropped the highly missing values and lets see how many columns left."
      ],
      "metadata": {
        "id": "J5AV41lBSCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of columns left in the dataset:\", pjtrn.shape[1])\n",
        "missing_percent = (pjtrn.isnull().sum() / len(pjtrn)) * 100\n",
        "\n",
        "low_missing = missing_percent[missing_percent > 0]\n",
        "print(\"Remaining columns with missing values:\")\n",
        "print(low_missing)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5q5tdsk0SCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am generating the list of columns with missing data."
      ],
      "metadata": {
        "id": "1rGWm-yWSCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cols = [\n",
        "    'trafficSource.isTrueDirect',\n",
        "    'trafficSource.keyword',\n",
        "    'pageViews',\n",
        "    'trafficSource.referralPath',\n",
        "    'totals.bounces',\n",
        "    'new_visits'\n",
        "]\n",
        "\n",
        "print(\"📌 Data types of missing columns:\")\n",
        "print(pjtrn[missing_cols].dtypes)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "wIKpxeDhSCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me sort them to categorical and numerical columns."
      ],
      "metadata": {
        "id": "avifmR8vSCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_dtypes = pjtrn[[\n",
        "    'trafficSource.isTrueDirect',\n",
        "    'trafficSource.keyword',\n",
        "    'pageViews',\n",
        "    'trafficSource.referralPath',\n",
        "    'totals.bounces',\n",
        "    'new_visits'\n",
        "]].dtypes\n",
        "\n",
        "numerical_missing_cols = missing_dtypes[missing_dtypes == 'float64'].index.tolist()\n",
        "print(\" Numerical columns with missing values:\")\n",
        "print(numerical_missing_cols)\n",
        "\n",
        "categorical_missing_cols = missing_dtypes[missing_dtypes == 'object'].index.tolist()\n",
        "print(\"\\n Categorical columns with missing values:\")\n",
        "print(categorical_missing_cols)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-6T2dfpLSCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cols = missing_percent[missing_percent > 0].index.tolist()\n",
        "\n",
        "categorical_missing = [col for col in missing_cols if pjtrn[col].dtype == 'object']\n",
        "numerical_missing   = [col for col in missing_cols if pjtrn[col].dtype in ['float64', 'int64']]\n",
        "boolean_missing     = [col for col in missing_cols if pjtrn[col].dtype == 'bool']\n",
        "\n",
        "print(\" Categorical Features with Missing Values:\")\n",
        "print(categorical_missing)\n",
        "\n",
        "print(\"\\n Numerical Features with Missing Values:\")\n",
        "print(numerical_missing)\n",
        "\n",
        "print(\"\\n Boolean Features with Missing Values:\")\n",
        "print(boolean_missing)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tAfX0XNnSCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imputing Missing Values\n",
        "\n",
        "As I dropped the highly missing value columns, next is to impute the rest of columns in a meaningful way. I will handle different types of features separately:\n",
        "\n",
        "Numerical columns ➝ Mean/Median\n",
        "\n",
        "Categorical columns ➝ Mode (most frequent value)\n",
        "\n",
        "Boolean columns ➝ Mode"
      ],
      "metadata": {
        "id": "fHTcr72eSCrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three categorical features, and three numerical features have to be imputed. Let me do the categorical first. Categorical values will be imputed with the mode of the column."
      ],
      "metadata": {
        "id": "vB37w0XBSCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_missing = ['trafficSource.isTrueDirect', 'trafficSource.keyword', 'trafficSource.referralPath']\n",
        "# Find and display the mode for each categorical column\n",
        "for col in categorical_missing:\n",
        "    mode_value = pjtrn[col].mode()[0]\n",
        "    print(f\"Mode of '{col}': {mode_value}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "LuP7rUIcSCrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me impute the values."
      ],
      "metadata": {
        "id": "ZV6C_du9SCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.fillna({\n",
        "    'trafficSource.isTrueDirect': True,\n",
        "    'trafficSource.keyword': '(not provided)',\n",
        "    'trafficSource.referralPath': '/'\n",
        "}, inplace=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KzzFeUpJSCrm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am gonna cross check once more."
      ],
      "metadata": {
        "id": "gbInC46-SCrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = pjtrn.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "missing_categorical = pjtrn[categorical_cols].isnull().sum()\n",
        "missing_categorical = missing_categorical[missing_categorical > 0]\n",
        "\n",
        "if missing_categorical.empty:\n",
        "    print(\" No missing values in categorical features.\")\n",
        "else:\n",
        "    print(\" Missing values still exist in these categorical columns:\")\n",
        "    print(missing_categorical)"
      ],
      "metadata": {
        "trusted": true,
        "id": "YwDu34fLSCrm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I am gonna impute the numerical values. Finding the mean, median and mode."
      ],
      "metadata": {
        "id": "h8wQafTUSCrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols_with_na = ['pageViews', 'totals.bounces', 'new_visits']\n",
        "\n",
        "for col in numerical_cols_with_na:\n",
        "    print(f\"\\n🔹 Column: {col}\")\n",
        "    print(f\"Mode   : {pjtrn[col].mode()[0]}\")\n",
        "    print(f\"Mean   : {pjtrn[col].mean()}\")\n",
        "    print(f\"Median : {pjtrn[col].median()}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "FFey0oDISCrm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing between mean, median, or mode for imputing missing values depends on the distribution of the data and the nature of the variable.\n",
        "1. Use Mean when:\n",
        "- The data is normally distributed (symmetric).\n",
        "- There are no extreme outliers.\n",
        "- When want a value that represents the arithmetic average.\n",
        "\n",
        "Good for: sensor readings, marks, heights, etc. Not good if there are spikes or extreme values (outliers).\n",
        "\n",
        "2. Use Median when:\n",
        "- The data is skewed, not symmetric.\n",
        "- There are outliers.\n",
        "- When want to use a robust measure that's not influenced by extremes.\n",
        "\n",
        "Best choice when the feature has 0s and a few very large numbers (like page views or bounces).\n",
        "Example: If most customers visit 1–2 pages but a few visit 100s, median is safer.\n",
        "\n",
        "3. Use Mode when:\n",
        "- The data is categorical or discrete with repeating values.\n",
        "- Imputing values like \"Yes\"/\"No\", 0/1, or specific categories.\n",
        "- Ideal for binary or categorical numerical-looking data.\n",
        "\n"
      ],
      "metadata": {
        "id": "4-gXvbZpSCrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_numerical_data = {\n",
        "    'Feature': ['pageViews', 'totals.bounces', 'new_visits'],\n",
        "    'Missing %': [0.006, 59.36, 30.60],\n",
        "    'Likely Data Type/Behavior': ['Count-like, skewed', 'Binary (0/1)', 'Binary (0/1)'],\n",
        "    'Best Imputation': ['Median', 'Mode', 'Mode']\n",
        "}\n",
        "\n",
        "imputation_plan_df = pd.DataFrame(missing_numerical_data)\n",
        "imputation_plan_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "xhVeCPzaSCrm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let me impute the values."
      ],
      "metadata": {
        "id": "CpfWA2pcSCrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn['pageViews'] = pjtrn['pageViews'].fillna(pjtrn['pageViews'].median())\n",
        "\n",
        "pjtrn['totals.bounces'] = pjtrn['totals.bounces'].fillna(pjtrn['totals.bounces'].mode()[0])\n",
        "\n",
        "pjtrn['new_visits'] = pjtrn['new_visits'].fillna(pjtrn['new_visits'].mode()[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "zGImLTqqSCrn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "pageViews is a continuous numeric variable (e.g., 1.0, 3.0, 15.0...). These kinds of variables can have skewed distributions (a few very large values). So, median is preferred over mean — it is robust to outliers and gives a better center value.\n",
        "Mode is better suited for categorical or binary values. For continuous values, it’s usually not meaningful.\n",
        "\n",
        "totals.bounces is imputed with Mode. Likely values: 0 or 1 (i.e., bounce or not). So this is binary categorical (even if it's in float format like 1.0 or 0.0).\n",
        "\n",
        "Mode is perfect here because it just fills missing values with the most frequent class (either 0 or 1).\n",
        "new_visits is also imputed with Mode. Same situation as totals.bounces. Likely values: 0 or 1 (new visit or not). This is also binary, and we just want to assign the most frequent value."
      ],
      "metadata": {
        "id": "WmRpE_vXSCrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me crosscheck once more for missing values."
      ],
      "metadata": {
        "id": "VfHUneTRSCrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn[['pageViews', 'totals.bounces', 'new_visits']].isnull().sum()"
      ],
      "metadata": {
        "trusted": true,
        "id": "m-9VKACfSCrn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Duplicates\n",
        "Next, I checked for the duplicate rows in the datasets."
      ],
      "metadata": {
        "id": "8gyN4XLBSCrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = pjtrn.duplicated()\n",
        "print(\"Total number of duplicated rows:\", duplicate_rows.sum())"
      ],
      "metadata": {
        "trusted": true,
        "id": "ugfIlKDySCrn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn[duplicate_rows].head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eVHPbsb3SCro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the duplicate rows."
      ],
      "metadata": {
        "id": "qnGEB17USCro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn.drop_duplicates(inplace=True)\n",
        "\n",
        "print(\"Duplicates removed. New dataset shape:\", pjtrn.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SRT0jBqMSCro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Outliers"
      ],
      "metadata": {
        "id": "MqDl-nNOSCro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "num_cols = ['pageViews', 'totals.bounces', 'new_visits', 'purchaseValue', 'totalHits']\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, col in enumerate(num_cols, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.boxplot(data=pjtrn, x=col)\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qZglT_z5SCro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_summary = {}\n",
        "\n",
        "for col in num_cols:\n",
        "    Q1 = pjtrn[col].quantile(0.25)\n",
        "    Q3 = pjtrn[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = pjtrn[(pjtrn[col] < lower) | (pjtrn[col] > upper)]\n",
        "    outlier_summary[col] = outliers.shape[0]\n",
        "    print(f\"{col}: {outliers.shape[0]} outliers\")\n",
        "\n",
        "import pandas as pd\n",
        "outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['# of Outliers'])\n",
        "outlier_df\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "FKaBFflLSCro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the outliers by capping"
      ],
      "metadata": {
        "id": "wzNyOPnvSCro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_pv = pjtrn['pageViews'].quantile(0.01)\n",
        "upper_pv = pjtrn['pageViews'].quantile(0.99)\n",
        "pjtrn['pageViews'] = pjtrn['pageViews'].clip(lower_pv, upper_pv)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bVAnBym4SCro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log transformation for the purchase value column values.\n",
        "1. Because the target variable is skewed\n",
        "- As seen in the histogram earlier, purchaseValue is highly right-skewed.\n",
        "- Skewed targets often hurt model performance, especially linear models and tree-based regressors.\n",
        "2. Log transformation helps by:\n",
        "- Compressing large values and reducing the effect of outliers.\n",
        "- Making the target distribution more normal-like (bell-shaped).\n",
        "- Helping models learn better relationships in the data."
      ],
      "metadata": {
        "id": "7vRK0VYiSCro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pjtrn['purchaseValue_log'] = np.log1p(pjtrn['purchaseValue'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "-PdAUKu3SCro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "lower_th = pjtrn['totalHits'].quantile(0.01)\n",
        "upper_th = pjtrn['totalHits'].quantile(0.99)\n",
        "pjtrn['totalHits'] = pjtrn['totalHits'].clip(lower_th, upper_th)"
      ],
      "metadata": {
        "trusted": true,
        "id": "leSgmmf4SCrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis\n",
        "EDA is the process of visually and statistically exploring the dataset to understand its structure, patterns, anomalies, and relationships between features before modeling.\n",
        "\n",
        "Why to do EDA?\n",
        "- To understand the data distribution\n",
        "- To detect outliers or inconsistencies\n",
        "- To identify patterns or trends\n",
        "- To see how features relate to the target variable\n",
        "- To decide what transformations or preprocessing might be needed\n",
        "\n",
        "Types of EDA\n",
        "I am going to explore data using two main approaches:\n",
        "\n",
        "1. Univariate Analysis - Studying one feature at a time\n",
        "\n",
        "- For categorical features: Value counts, bar plots\n",
        "- For numerical features: Histograms, box plots, summary statistics\n",
        "\n",
        "2. Bivariate/Multivariate Analysis - Studying two or more features together\n",
        "\n",
        "- Feature vs. Target (e.g., purchaseValue)\n",
        "- Correlation matrix between numerical features\n",
        "- Grouped aggregations\n",
        "\n",
        "Tools I’ll Use\n",
        "- pandas (for value counts and describe)\n",
        "- matplotlib and seaborn (for plotting)\n",
        "- plotly (for interactive graphs, if needed later)"
      ],
      "metadata": {
        "id": "SQ0YcNEFSCrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Select only numerical columns using .select_dtypes\n",
        "numerical_features = pjtrn.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Step 2: Check the list\n",
        "print(\"Numerical Features:\")\n",
        "print(numerical_features)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:24:20.572605Z",
          "iopub.execute_input": "2025-08-08T13:24:20.572905Z",
          "iopub.status.idle": "2025-08-08T13:24:20.587912Z",
          "shell.execute_reply.started": "2025-08-08T13:24:20.572882Z",
          "shell.execute_reply": "2025-08-08T13:24:20.586675Z"
        },
        "id": "xmm8sQm5SCrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = pjtrn.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"Categorical Features:\")\n",
        "print(categorical_features)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:24:20.588822Z",
          "iopub.execute_input": "2025-08-08T13:24:20.589142Z",
          "iopub.status.idle": "2025-08-08T13:24:20.602543Z",
          "shell.execute_reply.started": "2025-08-08T13:24:20.589117Z",
          "shell.execute_reply": "2025-08-08T13:24:20.601208Z"
        },
        "id": "G9DMCOQgSCrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "boolean_features = pjtrn.select_dtypes(include=['bool']).columns.tolist()\n",
        "print(\"Boolean Features:\")\n",
        "print(boolean_features)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "okX5RtFASCrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(pjtrn['purchaseValue'], bins=50, kde=True)\n",
        "plt.title(\"Distribution of Purchase Value\")\n",
        "plt.xlabel(\"Purchase Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "4jOEljmySCrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=pjtrn['purchaseValue'])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "LdTrQXJMSCrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "dco7EcuASCrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_summary = {\n",
        "    'Numerical': len(numerical_features),\n",
        "    'Categorical': len(categorical_features),\n",
        "    'Boolean': len(boolean_features)\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame.from_dict(feature_summary, orient='index', columns=['Count'])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Xt9dYPs4SCrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn[numerical_features].describe().T"
      ],
      "metadata": {
        "trusted": true,
        "id": "vFGEBZoqSCrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting UNIX timestamp to date and time"
      ],
      "metadata": {
        "id": "eOcaKSM_SCrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pjtrn['session_date'] = pd.to_datetime(pjtrn['sessionStart'], unit='s')\n",
        "\n",
        "pjtrn['session_hour'] = pjtrn['session_date'].dt.hour\n",
        "pjtrn['session_weekday'] = pjtrn['session_date'].dt.weekday\n",
        "pjtrn['session_month'] = pjtrn['session_date'].dt.month\n",
        "\n",
        "pjtrn[['sessionStart', 'session_date', 'session_hour', 'session_weekday', 'session_month']].head()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZEBWzF-CSCrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn['has_keyword'] = pjtrn['trafficSource.keyword'].apply(lambda x: 0 if pd.isnull(x) or x == '(not provided)' else 1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4Q9JcEKZSCrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn['has_referral'] = pjtrn['trafficSource.referralPath'].notnull().astype(int)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NuPH6OGSSCrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "res_split = pjtrn['device.screenResolution'].str.extract(r'(?P<screen_width>\\d+)x(?P<screen_height>\\d+)')\n",
        "\n",
        "pjtrn['screen_width'] = pd.to_numeric(res_split['screen_width'], errors='coerce')\n",
        "pjtrn['screen_height'] = pd.to_numeric(res_split['screen_height'], errors='coerce')"
      ],
      "metadata": {
        "trusted": true,
        "id": "889GURXSSCrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn['browser_major_version'] = pjtrn['device.browserVersion'].str.extract(r'(\\d+)').astype(float)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a_TlrDWBSCrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "screen_split = pjtrn['screenSize'].str.extract(r'(?P<screen_width_px>\\d+)x(?P<screen_height_px>\\d+)')\n",
        "pjtrn['screen_width_px'] = pd.to_numeric(screen_split['screen_width_px'], errors='coerce')\n",
        "pjtrn['screen_height_px'] = pd.to_numeric(screen_split['screen_height_px'], errors='coerce')"
      ],
      "metadata": {
        "trusted": true,
        "id": "2Ldn2Mu8SCrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn['device.isMobile'] = pjtrn['device.isMobile'].astype(int)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BrXXfYi1SCrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "low_card_cols = ['deviceType', 'browser', 'userChannel', 'geoNetwork.continent', 'os']\n",
        "le = LabelEncoder()\n",
        "for col in low_card_cols:\n",
        "    pjtrn[col] = le.fit_transform(pjtrn[col])"
      ],
      "metadata": {
        "trusted": true,
        "id": "fuzFw62iSCrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtrn = pd.get_dummies(pjtrn, columns=['device.language'], drop_first=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "PibNYpdKSCrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation Analysis\n"
      ],
      "metadata": {
        "id": "yntGGpG6SCrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "numerical_cols = pjtrn.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "corr_matrix = pjtrn[numerical_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix of Numerical Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "alcg6EVOSCrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection based on Correlation\n",
        "\n",
        "To improve model performance and reduce noise, I computed the Pearson correlation of all numerical features with the target (`purchaseValue_log`). Features with correlation ≥ 0.1 were retained. This helped in:\n",
        "\n",
        "- Reducing irrelevant or noisy features\n",
        "- Speeding up model training\n",
        "- Improving model interpretability\n",
        "\n",
        "The final feature set includes: `totalHits`, `pageViews`, `new_visits`, etc.\n"
      ],
      "metadata": {
        "id": "w7z-hrsiSCrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_corr = corr_matrix['purchaseValue_log'].sort_values(ascending=False)\n",
        "print(\"🔍 Top correlations with purchaseValue_log:\")\n",
        "print(target_corr)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "zWcGo-wCSCrs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What I am doing:\n",
        "Get correlations with target\n",
        "Finding Absolute correlation (to catch both positive and negative)\n",
        "Select features with at least 0.1 correlation\n",
        "Remove the target itself\n"
      ],
      "metadata": {
        "id": "_f6iwSe6SCrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_corr = corr_matrix['purchaseValue_log']\n",
        "\n",
        "\n",
        "abs_corr = target_corr.abs()\n",
        "\n",
        "\n",
        "selected_features = abs_corr[abs_corr >= 0.1].index.tolist()\n",
        "\n",
        "\n",
        "selected_features.remove('purchaseValue_log')\n",
        "\n",
        "print(\"📌 Selected features based on correlation:\")\n",
        "print(selected_features)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0cr5qAF0SCrs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I have the Final dataset for modeling"
      ],
      "metadata": {
        "id": "v3eJkG5mSCrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pjtrn[selected_features]\n",
        "y = pjtrn['purchaseValue_log']"
      ],
      "metadata": {
        "trusted": true,
        "id": "1ac_JX-MSCrs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the Dataset\n",
        "X = my selected features from before\n",
        "\n",
        "y = target variable (purchaseValue_log)"
      ],
      "metadata": {
        "id": "dc9eqZhASCrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "exSi_7RISCrs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "Model training is the process where a machine learning algorithm learns patterns from the input features (X_train) and their corresponding target values (y_train). The model adjusts its internal parameters to minimize the error in predictions.\n",
        "\n",
        "By training models on historical data, we enable them to make accurate predictions on unseen (test) data. A well-trained model captures meaningful patterns and generalizes well.\n",
        "\n",
        "I will train multiple regression models such as Linear Regression, Decision Tree, Random Forest, and XGBoost. Each model will be evaluated using metrics like RMSE and R². Later, I’ll fine-tune the best-performing models using GridSearchCV for better accuracy.\n",
        "### Train a Baseline Model - Linear Regression"
      ],
      "metadata": {
        "id": "xPg-AZ2USCrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"🔍 Linear Regression Results:\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R² Score:\", r2)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "AktHiIeLSCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a Tree-Based Model - Random Forest"
      ],
      "metadata": {
        "id": "Cm9X1Y1FSCrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_rf_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_rf_pred))\n",
        "r2_rf = r2_score(y_test, y_rf_pred)\n",
        "\n",
        "print(\"🌳 Random Forest Results:\")\n",
        "print(\"RMSE:\", rmse_rf)\n",
        "print(\"R² Score:\", r2_rf)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "2Lz0Ku1tSCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n📊 Model Comparison\")\n",
        "print(f\"Linear Regression → RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
        "print(f\"Random Forest     → RMSE: {rmse_rf:.4f}, R²: {r2_rf:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "A7AEXCm9SCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🔍 Features used in training:\")\n",
        "print(X_train.columns.tolist())"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ak2gH_g5SCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X = pjtrn[selected_features].copy()\n",
        "y = pjtrn['purchaseValue_log'].copy()\n",
        "\n",
        "# Ensure 'purchaseValue' and related columns are not in X\n",
        "print(X.columns)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "mI8aPs6iSCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check highest pairwise correlation\n",
        "abs_corr_matrix = X.corr().abs()\n",
        "np.fill_diagonal(abs_corr_matrix.values, 0)  # Ignore self-correlation\n",
        "\n",
        "highly_corr_pairs = abs_corr_matrix[abs_corr_matrix > 0.98].stack()\n",
        "print(highly_corr_pairs)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "u5LW3eTwSCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_rf_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_rf_pred))\n",
        "r2_rf = r2_score(y_test, y_rf_pred)\n",
        "\n",
        "print(\"🌳 Random Forest Results:\")\n",
        "print(\"RMSE:\", rmse_rf)\n",
        "print(\"R² Score:\", r2_rf)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vn11GOjCSCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "While evaluating the Random Forest model, I initially observed an R² score of 1.0 — which was unusually perfect. Upon inspection, I found that the original `purchaseValue` column (which is the target itself) was mistakenly included as a feature.\n",
        "\n",
        "This led to data leakage, causing the model to \"cheat\" and memorize the answer. After removing the leaked feature, the model performance was re-evaluated to reflect realistic results.\n",
        "\n",
        "So,\n",
        "- I dropped 'purchaseValue' from X.\n",
        "- Used log-transformed purchase value as target\n",
        "- And then split the train and test\n"
      ],
      "metadata": {
        "id": "EMeOGL7pSCrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = pjtrn[selected_features].drop(columns=['purchaseValue'], errors='ignore')\n",
        "\n",
        "y = pjtrn['purchaseValue_log']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q-wL1NhZSCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_rf_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_rf_pred))\n",
        "r2_rf = r2_score(y_test, y_rf_pred)\n",
        "\n",
        "print(\"🌳 Random Forest Results:\")\n",
        "print(\"RMSE:\", rmse_rf)\n",
        "print(\"R² Score:\", r2_rf)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hr528x1LSCrt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation using Pipelines\n",
        "\n",
        "To ensure clean and scalable modeling, I used Scikit-learn Pipelines for all 7 regression models. This allowed me to include preprocessing steps (like Standard Scaling) for models like SVR and KNN, while skipping it for tree-based models that don’t require it.\n",
        "\n",
        "The pipeline ensured:\n",
        "- Cleaner code\n",
        "- Avoidance of data leakage\n",
        "- Reproducible model comparisons\n",
        "\n",
        "Each pipeline was trained and evaluated using RMSE and R² metrics.\n"
      ],
      "metadata": {
        "id": "zLnw_RtNSCru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Use Pipelines?\n",
        "\n",
        "Pipelines are a best practice in machine learning to streamline the workflow and ensure consistency.\n",
        "\n",
        "Pipelines\n",
        "- Avoids data leakage by applying transformations only to training data\n",
        "- Ensures cleaner, modular, and reproducible code\n",
        "- Allows preprocessing and modeling to be bundled together\n",
        "- Supports hyperparameter tuning across all steps using GridSearchCV\n",
        "\n",
        "In this project, pipelines were especially useful for models like SVR and KNN which require scaling, while avoiding unnecessary preprocessing for tree-based models.\n"
      ],
      "metadata": {
        "id": "bqDpD7ZPSCru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "\n",
        "models = {\n",
        "    \"Linear Regression\": Pipeline([('model', LinearRegression())]),\n",
        "    \"Random Forest\": Pipeline([('model', RandomForestRegressor(random_state=42))]),\n",
        "    \"Decision Tree\": Pipeline([('model', DecisionTreeRegressor(random_state=42))]),\n",
        "    \"KNN\": Pipeline([('scaler', StandardScaler()), ('model', KNeighborsRegressor())]),\n",
        "    \"SVR\": Pipeline([('scaler', StandardScaler()), ('model', SVR())]),\n",
        "    \"Gradient Boosting\": Pipeline([('model', GradientBoostingRegressor(random_state=42))]),\n",
        "    \"XGBoost\": Pipeline([('model', XGBRegressor(random_state=42, verbosity=0))])\n",
        "}\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for name, pipeline in models.items():\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    model_results[name] = {\"RMSE\": rmse, \"R2\": r2}\n",
        "\n",
        "print(\"\\n📊 Model Performance Comparison (via Pipeline)\")\n",
        "for name, result in model_results.items():\n",
        "    print(f\"{name:<20} → RMSE: {result['RMSE']:.4f}, R²: {result['R2']:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_FU9lCc-SCru"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler beacuse it standardizes the data by:\n",
        "- Centering the mean at 0\n",
        "- Scaling the variance to 1\n",
        "This means it transforms numerical features so they all contribute equally during distance or margin-based calculations.\n",
        "\n",
        "\n",
        "K-Nearest Neighbors (KNN):\n",
        "KNN is a distance-based algorithm (uses Euclidean or similar distance). If one feature (like pageViews) has a larger range than another (like new_visits), it will dominate the distance metric. StandardScaler ensures all features contribute equally to the distance.\n",
        "\n",
        "Support Vector Regressor (SVR):\n",
        "SVR tries to find the best margin (hyperplane) in the feature space. It is sensitive to the scale of input features when calculating support vectors. Unscaled data can distort the kernel function and margin calculations. So scaling is mandatory for SVR to perform well.\n",
        "\n",
        "Why I did not scale for other models is that they are tree-based models (Decision Tree, Random Forest, Gradient Boosting, XGBoost). These models are not sensitive to feature scale. They split nodes based on feature thresholds, not distances. Scaling doesn't affect how the splits are made, so it's not needed.\n",
        "\n",
        "In Linear Regression, While scaling can help interpret coefficients, it’s not mandatory for performance. Can add scaling if using regularization (like Ridge, Lasso), but here it’s plain Linear Regression.\n",
        "\n",
        "So, I applied StandardScaler only to models like KNN and SVR because they are sensitive to feature scales, unlike tree-based models which are scale-invariant."
      ],
      "metadata": {
        "id": "ypGgZGpMSCru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Performance Comparison\n",
        "\n",
        "I evaluated seven regression models using RMSE and R² metrics via Scikit-learn Pipelines. Here's a summary of the results:\n",
        "\n",
        "- **XGBoost** delivered the best performance with RMSE of 3.3882 and R² of 0.7796.\n",
        "- Tree-based ensemble models (Random Forest, Gradient Boosting, XGBoost) outperformed others significantly.\n",
        "- **Linear Regression** performed the poorest, likely due to its assumption of linearity, which doesn’t hold well for this dataset.\n",
        "\n",
        "These insights suggest that ensemble tree models are better suited for predicting `purchaseValue_log` in this case.\n"
      ],
      "metadata": {
        "id": "wHt-vXtNSCru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Hyperparameter Tuning\n",
        "\n",
        "To improve model performance, I applied `GridSearchCV` to fine-tune the hyperparameters of the top-performing models:\n",
        "\n",
        "- **Random Forest**\n",
        "- **Gradient Boosting**\n",
        "- **XGBoost**\n",
        "\n",
        "This method uses cross-validation to evaluate different combinations of parameters and selects the one with the best RMSE score.\n",
        "\n",
        "**Advantages of Hyperparameter Tuning:**\n",
        "- Helps avoid overfitting or underfitting\n",
        "- Enhances generalization to unseen data\n",
        "- Boosts the overall predictive performance\n"
      ],
      "metadata": {
        "id": "dKZ0Zr1JSCru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models to Tune:\n",
        "-Random Forest\n",
        "-Gradient Boosting\n",
        "-XGBoost\n",
        "\n",
        "Why these?\n",
        "- They’re already top performers.\n",
        "- They benefit a lot from hyperparameter tuning.\n",
        "- And they’re all tree-based → so can learn a common tuning pattern."
      ],
      "metadata": {
        "id": "_5ON2YVqSCru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define pipeline\n",
        "rf_pipe = Pipeline([\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Define parameter grid\n",
        "rf_param_grid = {\n",
        "    'model__n_estimators': [50, 100],\n",
        "    'model__max_depth': [None, 10, 20],\n",
        "    'model__min_samples_split': [2, 5],\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "rf_grid = GridSearchCV(estimator=rf_pipe,\n",
        "                       param_grid=rf_param_grid,\n",
        "                       scoring='neg_root_mean_squared_error',\n",
        "                       cv=5,\n",
        "                       n_jobs=-1,\n",
        "                       verbose=1)\n",
        "\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "# Best score and parameters\n",
        "print(\"🔍 Best RF RMSE (CV):\", -rf_grid.best_score_)\n",
        "print(\"🏆 Best RF Parameters:\", rf_grid.best_params_)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ULt6s9S6SCru"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV exhaustively tries all combinations of hyperparameters you provide in the grid.For each combination, it performs cross-validation (here 5-fold) to measure model performance.It returns the combination that gives the best performance score (in this case, lowest RMSE).\n",
        "Small search space:\n",
        "\n",
        "My parameter grid is relatively small:\n",
        "n_estimators: 2 options (50, 100)\n",
        "max_depth: 3 options (None, 10, 20)\n",
        "min_samples_split: 2 options (2, 5)\n",
        "→ 2 × 3 × 2 = 12 combinations\n",
        "Since the total number of combinations is small (12), an exhaustive search is feasible and reliable.\n",
        "GridSearch ensures that every single combination is evaluated, which gives me confidence that the selected hyperparameters are truly optimal from the options I chose. Since all combinations are tested systematically, results are deterministic and reproducible, especially useful when using random_state.\n",
        "\n",
        "RandomizedSearchCV randomly selects a subset of combinations from the parameter space. It is useful when the search space is huge or training is time-consuming. But in my case, The grid was small. I wanted an exhaustive and complete evaluation. So, I didn't need the randomness or approximation of RandomizedSearchCV."
      ],
      "metadata": {
        "id": "u7TmAl5JSCru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb_pipe = Pipeline([\n",
        "    ('model', GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "gb_param_grid = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__learning_rate': [0.05, 0.1],\n",
        "    'model__max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "gb_grid = GridSearchCV(estimator=gb_pipe,\n",
        "                       param_grid=gb_param_grid,\n",
        "                       scoring='neg_root_mean_squared_error',\n",
        "                       cv=5,\n",
        "                       n_jobs=-1,\n",
        "                       verbose=1)\n",
        "\n",
        "gb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"🔍 Best GB RMSE (CV):\", -gb_grid.best_score_)\n",
        "print(\"🏆 Best GB Parameters:\", gb_grid.best_params_)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "FI1_tmk2SCrv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🎯 Hyperparameter Tuning: Gradient Boosting\n",
        "\n",
        "To improve the performance of Gradient Boosting Regressor, I performed hyperparameter tuning using GridSearchCV with 5-fold cross-validation.\n",
        "\n",
        "The grid included:\n",
        "- `n_estimators`: [100, 200]\n",
        "- `learning_rate`: [0.05, 0.1]\n",
        "- `max_depth`: [3, 5]\n",
        "\n",
        "**Best RMSE (CV):** 3.3855  \n",
        "**Best Parameters:**\n",
        "- learning_rate = 0.1\n",
        "- max_depth = 5\n",
        "- n_estimators = 100\n",
        "\n",
        "These values strike a balance between underfitting and overfitting, leading to a more generalizable model.\n"
      ],
      "metadata": {
        "id": "9D5XUYPXSCrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_pipe = Pipeline([\n",
        "    ('model', XGBRegressor(random_state=42, verbosity=0))\n",
        "])\n",
        "\n",
        "xgb_param_grid = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__learning_rate': [0.05, 0.1],\n",
        "    'model__max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(estimator=xgb_pipe,\n",
        "                        param_grid=xgb_param_grid,\n",
        "                        scoring='neg_root_mean_squared_error',\n",
        "                        cv=5,\n",
        "                        n_jobs=-1,\n",
        "                        verbose=1)\n",
        "\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"🔍 Best XGB RMSE (CV):\", -xgb_grid.best_score_)\n",
        "print(\"🏆 Best XGB Parameters:\", xgb_grid.best_params_)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HSHHrYIMSCrv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🎯 Hyperparameter Tuning: XGBoost Regressor\n",
        "\n",
        "XGBoost was tuned using GridSearchCV with 5-fold cross-validation.\n",
        "\n",
        "**Parameter grid included:**\n",
        "- `n_estimators`: [100, 200]\n",
        "- `learning_rate`: [0.05, 0.1]\n",
        "- `max_depth`: [3, 5]\n",
        "\n",
        "**Best RMSE (CV):** 3.3849  \n",
        "**Best Parameters:**\n",
        "- learning_rate = 0.05\n",
        "- max_depth = 5\n",
        "- n_estimators = 200\n",
        "\n",
        "XGBoost emerged as the best-performing model in cross-validation, slightly edging out Gradient Boosting.\n"
      ],
      "metadata": {
        "id": "5V1CQqsFSCrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_best = xgb_grid.best_estimator_\n",
        "y_pred_xgb = xgb_best.predict(X_test)\n",
        "\n",
        "rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"📊 XGBoost (Tuned) Test Results:\")\n",
        "print(\"RMSE:\", rmse_xgb)\n",
        "print(\"R² Score:\", r2_xgb)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Zx8cLJeYSCrv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_pipeline = Pipeline([\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "rf_param_grid = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [5, 10],\n",
        "    'model__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    rf_pipeline,\n",
        "    rf_param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"🔍 Best RF RMSE (CV):\", -rf_grid.best_score_)\n",
        "print(\"🏆 Best RF Parameters:\", rf_grid.best_params_)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "BXo6LemPSCrv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_pipe = Pipeline([\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "rf_params = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [None, 10, 20],\n",
        "    'model__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    rf_pipe,\n",
        "    param_grid=rf_params,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "rf_grid.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "L1asq2iTSCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Best RF Parameters:\", rf_grid.best_params_)\n",
        "\n",
        "best_rmse = -rf_grid.best_score_\n",
        "print(\"Best RF RMSE (CV):\", best_rmse)\n",
        "\n",
        "best_rf_model = rf_grid.best_estimator_\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "F9hUQBj-SCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "rmse_test = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2_test = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test RMSE:\", rmse_test)\n",
        "print(\"Test R² Score:\", r2_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "zaBkiofYSCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on the Real Test Data Set"
      ],
      "metadata": {
        "id": "XPCj51ZPSCrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pjtest = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
        "\n",
        "pjtest.shape, pjtest.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6GXdO1u6SCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation done same as training data set."
      ],
      "metadata": {
        "id": "hYkzivGvSCrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['trafficSource.isTrueDirect', 'trafficSource.keyword', 'trafficSource.referralPath']:\n",
        "    pjtest[col] = pjtest[col].fillna(pjtrn[col].mode()[0])\n",
        "\n",
        "pjtest['pageViews'] = pjtest['pageViews'].fillna(pjtrn['pageViews'].median())\n",
        "pjtest['totals.bounces'] = pjtest['totals.bounces'].fillna(pjtrn['totals.bounces'].mode()[0])\n",
        "pjtest['new_visits'] = pjtest['new_visits'].fillna(pjtrn['new_visits'].mode()[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_jsjqFa7SCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for col in ['trafficSource.isTrueDirect', 'trafficSource.keyword', 'trafficSource.referralPath']:\n",
        "    pjtest[col] = pjtest[col].fillna(pjtrn[col].mode()[0])\n",
        "\n",
        "pjtest['pageViews'] = pjtest['pageViews'].fillna(pjtrn['pageViews'].median())\n",
        "pjtest['totals.bounces'] = pjtest['totals.bounces'].fillna(pjtrn['totals.bounces'].mode()[0])\n",
        "pjtest['new_visits'] = pjtest['new_visits'].fillna(pjtrn['new_visits'].mode()[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Sd9SnjcTSCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtest['pageViews_log'] = np.log1p(pjtest['pageViews'])\n",
        "pjtest['totalHits_log'] = np.log1p(pjtest['totalHits'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "y_UXN9GuSCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['browser', 'os', 'deviceType', 'geoNetwork.continent']:\n",
        "    pjtest[col] = pjtest[col].map(pjtrn[col].value_counts().index.to_series().reset_index(drop=True).to_dict()).fillna(-1)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "plKIkj28SCrw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtest_encoded = pd.get_dummies(pjtest)\n",
        "pjtest_encoded = pjtest_encoded.reindex(columns=X_train.columns, fill_value=0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "14cNNr1HSCrx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "xgb_final = Pipeline([\n",
        "    ('model', XGBRegressor(\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        n_estimators=200,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_final.fit(X, y)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ExMfzExhSCrx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pjtest_preds_log = xgb_final.predict(pjtest_encoded)\n",
        "\n",
        "pjtest_preds = np.expm1(pjtest_preds_log)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NRy8SWx7SCrx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_predictions = xgb_final.predict(pjtest_encoded)\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sessionId': pjtest['sessionId'],\n",
        "    'purchaseValue': final_predictions\n",
        "})\n",
        "\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': range(len(pjtest['userId'])),\n",
        "    'purchaseValue': final_predictions\n",
        "})\n",
        "\n",
        "\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\" Submission file created: submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "urcRC1D8SCrx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'userId': pjtest['userId'],\n",
        "    'purchaseValue': pjtest_preds\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head()\n",
        "print(\" Submission file created: submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "sKUPyIjxSCrx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "spfkkvtQSCrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0TOeyhwISCrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IFx5Xt4wSCrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Load your training data\n",
        "df = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\")\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Function to classify distribution\n",
        "def classify_distribution(skew_val):\n",
        "    if pd.isna(skew_val):\n",
        "        return \"Likely Constant\"\n",
        "    elif abs(skew_val) < 0.5:\n",
        "        return \"Approximately Normal\"\n",
        "    elif skew_val > 0:\n",
        "        return \"Right Skewed\"\n",
        "    else:\n",
        "        return \"Left Skewed\"\n",
        "\n",
        "# Create results table\n",
        "skew_results = []\n",
        "for col in numeric_cols:\n",
        "    skew_val = skew(df[col].dropna())\n",
        "    dist_type = classify_distribution(skew_val)\n",
        "\n",
        "    # Suggest handling\n",
        "    if dist_type == \"Right Skewed\" and abs(skew_val) > 1:\n",
        "        action = \"Consider log/sqrt transformation\"\n",
        "    elif dist_type == \"Left Skewed\" and abs(skew_val) > 1:\n",
        "        action = \"Consider square/power transformation\"\n",
        "    elif dist_type == \"Likely Constant\":\n",
        "        action = \"Drop column (no variation)\"\n",
        "    else:\n",
        "        action = \"No transformation needed\"\n",
        "\n",
        "    skew_results.append({\n",
        "        \"Column\": col,\n",
        "        \"Skewness\": round(skew_val, 4) if not pd.isna(skew_val) else None,\n",
        "        \"Distribution\": dist_type,\n",
        "        \"Recommended Action\": action\n",
        "    })\n",
        "\n",
        "skew_df = pd.DataFrame(skew_results)\n",
        "\n",
        "# Show table\n",
        "print(\"\\n📊 Skewness Summary Table\")\n",
        "display(skew_df)\n",
        "\n",
        "# Plot histograms for each numeric column\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(5,3))\n",
        "    sns.histplot(df[col].dropna(), kde=True)\n",
        "    plt.title(f\"{col} Distribution\\nSkewness: {skew(df[col].dropna()):.2f}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kLWnXjjmSCrx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\")\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Function to apply right transformation\n",
        "def transform_column(series, skew_val):\n",
        "    if pd.isna(skew_val) or series.nunique() <= 1:\n",
        "        return None, \"Drop column (no variation)\"\n",
        "    elif abs(skew_val) < 0.5:\n",
        "        return series, \"No transformation needed\"\n",
        "    elif skew_val > 1:\n",
        "        return np.log1p(series), \"Log(x+1) transformation\"\n",
        "    elif 0.5 < skew_val <= 1:\n",
        "        return np.sqrt(series), \"Square root transformation\"\n",
        "    elif skew_val < -1:\n",
        "        return np.power(series, 2), \"Square transformation\"\n",
        "    elif -1 <= skew_val < -0.5:\n",
        "        return np.power(series, 3), \"Cube transformation\"\n",
        "    else:\n",
        "        return series, \"No transformation applied\"\n",
        "\n",
        "# Store transformation summary\n",
        "transform_summary = []\n",
        "df_transformed = df.copy()\n",
        "\n",
        "for col in numeric_cols:\n",
        "    original_skew = skew(df[col].dropna())\n",
        "    transformed_series, action = transform_column(df[col].dropna(), original_skew)\n",
        "\n",
        "    if transformed_series is None:\n",
        "        df_transformed.drop(columns=[col], inplace=True)\n",
        "        transformed_skew = None\n",
        "    else:\n",
        "        df_transformed[col] = transformed_series\n",
        "        transformed_skew = skew(transformed_series.dropna())\n",
        "\n",
        "        # Plot before and after\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "        sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
        "        axes[0].set_title(f\"{col} - Before\\nSkew: {original_skew:.2f}\")\n",
        "        sns.histplot(transformed_series.dropna(), kde=True, ax=axes[1])\n",
        "        axes[1].set_title(f\"{col} - After\\nSkew: {transformed_skew:.2f}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    transform_summary.append({\n",
        "        \"Column\": col,\n",
        "        \"Original Skewness\": round(original_skew, 4) if not pd.isna(original_skew) else None,\n",
        "        \"Transformation Applied\": action,\n",
        "        \"Transformed Skewness\": round(transformed_skew, 4) if transformed_skew is not None else None\n",
        "    })\n",
        "\n",
        "# Create and display summary table\n",
        "transform_df = pd.DataFrame(transform_summary)\n",
        "display(transform_df)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Kag0p-2-SCrx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# --------------------\n",
        "# 📍 MILESTONE 1\n",
        "# --------------------\n",
        "\n",
        "# ✅ Step 1: Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ✅ Step 2: Load dataset\n",
        "pjtrn = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\")\n",
        "\n",
        "# ✅ Step 3: Basic Info\n",
        "print(\"Shape of training data:\", pjtrn.shape)\n",
        "print(\"\\nColumns in the dataset:\\n\", pjtrn.columns.tolist())\n",
        "print(\"\\nData types:\\n\", pjtrn.dtypes)\n",
        "print(\"\\nSample rows:\\n\")\n",
        "pjtrn.head()\n",
        "\n",
        "# ✅ Step 4: Categorize feature types\n",
        "dtype_map = pjtrn.dtypes\n",
        "feature_type_df = pd.DataFrame({\n",
        "    'Feature': pjtrn.columns,\n",
        "    'Data Type': dtype_map.values,\n",
        "})\n",
        "\n",
        "categorical_features = feature_type_df[feature_type_df['Data Type'] == 'object']['Feature'].tolist()\n",
        "numerical_features = feature_type_df[feature_type_df['Data Type'].isin(['int64', 'float64'])]['Feature'].tolist()\n",
        "boolean_features = feature_type_df[feature_type_df['Data Type'] == 'bool']['Feature'].tolist()\n",
        "\n",
        "# ✅ Step 5: Identify missing values\n",
        "missing_check = pjtrn.isnull()\n",
        "missing_count = missing_check.sum()\n",
        "missing_values_df = missing_count.to_frame(name='Missing Values')\n",
        "missing_values_df['% of Total Values'] = (missing_values_df['Missing Values'] / len(pjtrn)) * 100\n",
        "missing_values_df = missing_values_df[missing_values_df['Missing Values'] > 0]\n",
        "missing_values_df.reset_index(inplace=True)\n",
        "missing_values_df.rename(columns={'index': 'Feature'}, inplace=True)\n",
        "missing_values_df = missing_values_df.sort_values(by='Missing Values', ascending=False)\n",
        "\n",
        "# ✅ Step 6: Impute missing categorical features with mode\n",
        "pjtrn['trafficSource.isTrueDirect'] = pjtrn['trafficSource.isTrueDirect'].fillna(True)\n",
        "pjtrn['trafficSource.keyword'] = pjtrn['trafficSource.keyword'].fillna('(not provided)')\n",
        "pjtrn['trafficSource.referralPath'] = pjtrn['trafficSource.referralPath'].fillna('/')\n",
        "\n",
        "# ✅ Step 7: Impute numerical features with mean\n",
        "pjtrn['pageViews'] = pjtrn['pageViews'].fillna(pjtrn['pageViews'].mean())\n",
        "pjtrn['totals.bounces'] = pjtrn['totals.bounces'].fillna(pjtrn['totals.bounces'].mean())\n",
        "pjtrn['new_visits'] = pjtrn['new_visits'].fillna(pjtrn['new_visits'].mean())\n",
        "\n",
        "# ✅ Step 8: EDA - Descriptive Statistics\n",
        "pjtrn.describe(include='all').T\n",
        "\n",
        "# ✅ Step 9: EDA - Visualizations\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(pjtrn['purchaseValue'], bins=50, kde=True)\n",
        "plt.title(\"Distribution of Purchase Value\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(x=pjtrn['purchaseValue'])\n",
        "plt.title(\"Boxplot of Purchase Value\")\n",
        "plt.show()\n",
        "\n",
        "# ✅ Step 10: Train-Validation Split\n",
        "X = pjtrn.drop(['purchaseValue'], axis=1)\n",
        "y = pjtrn['purchaseValue']\n",
        "\n",
        "# Drop high-missing-value features before modeling (optional)\n",
        "high_missing = missing_values_df[missing_values_df['% of Total Values'] > 90]['Feature'].tolist()\n",
        "X = X.drop(columns=high_missing)\n",
        "\n",
        "# Simple encoding for categorical columns (optional early baseline)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ✅ Step 11: Baseline model (Linear Regression)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred = lr_model.predict(X_val)\n",
        "\n",
        "print(\"\\nBaseline Model Evaluation:\")\n",
        "print(\"MSE:\", mean_squared_error(y_val, y_pred))\n",
        "print(\"R^2 Score:\", r2_score(y_val, y_pred))\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "xwOSHMvHSCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Milestone 2"
      ],
      "metadata": {
        "id": "gqCj3CTNSCry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Impute remaining numerical NaN values with median\n",
        "X = X.fillna(X.median())\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ux-tcWLrSCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Drop target column to get features\n",
        "X = pjtrn.drop(columns=['purchaseValue'])\n",
        "\n",
        "# Select only numerical features\n",
        "X = X.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Fill remaining missing values in numerical features\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# Target variable\n",
        "y = pjtrn['purchaseValue']\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "a1MfG23LSCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Linear Regression Model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "y_pred_lr = lin_reg.predict(X)\n",
        "\n",
        "# Performance Metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse_lr = mean_squared_error(y, y_pred_lr)\n",
        "r2_lr = r2_score(y, y_pred_lr)\n",
        "\n",
        "print(\"Linear Regression Results:\")\n",
        "print(f\"Mean Squared Error: {mse_lr}\")\n",
        "print(f\"R2 Score: {r2_lr}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "aH41iFaOSCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ----------- 2. Stochastic Gradient Descent Regressor -----------\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "sgd_reg.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_pred_sgd = sgd_reg.predict(X)\n",
        "\n",
        "# Evaluate\n",
        "mse_sgd = mean_squared_error(y, y_pred_sgd)\n",
        "r2_sgd = r2_score(y, y_pred_sgd)\n",
        "\n",
        "print(\"SGD Regressor Results:\")\n",
        "print(\"Mean Squared Error:\", mse_sgd)\n",
        "print(\"R2 Score:\", r2_sgd)\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "Kj2q3CE1SCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "mldRogwPSCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_reg.fit(X_scaled, y)\n",
        "y_pred_sgd = sgd_reg.predict(X_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mse_sgd = mean_squared_error(y, y_pred_sgd)\n",
        "r2_sgd = r2_score(y, y_pred_sgd)\n",
        "\n",
        "print(\"Scaled SGD Regressor Results:\")\n",
        "print(\"Mean Squared Error:\", mse_sgd)\n",
        "print(\"R2 Score:\", r2_sgd)\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "imZFUNQaSCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Create a pipeline with scaling + SGD\n",
        "sgd_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('sgd', SGDRegressor(random_state=42, max_iter=1000, tol=1e-3))\n",
        "])\n",
        "\n",
        "# 2. Define parameter grid\n",
        "param_grid = {\n",
        "    'sgd__alpha': [0.0001, 0.001, 0.01],               # Regularization strength\n",
        "    'sgd__penalty': ['l2', 'l1', 'elasticnet'],        # Type of penalty\n",
        "    'sgd__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
        "    'sgd__eta0': [0.001, 0.01, 0.1]                    # Initial learning rate\n",
        "}\n",
        "\n",
        "# 3. Grid search\n",
        "grid_search = GridSearchCV(sgd_pipeline, param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# 4. Best parameters and score\n",
        "print(\"Best Parameters:\\n\", grid_search.best_params_)\n",
        "print(\"Best R2 Score from CV:\", grid_search.best_score_)\n",
        "\n",
        "# 5. Evaluate on full training data\n",
        "best_sgd_model = grid_search.best_estimator_\n",
        "y_pred_grid = best_sgd_model.predict(X)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "print(\"Final SGD on Full Data\")\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_grid))\n",
        "print(\"R²:\", r2_score(y, y_pred_grid))\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "IENIoeDPSCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Split Data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Scale the Data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# ------------------------------\n",
        "# 🌲 Random Forest Regressor\n",
        "# ------------------------------\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "\n",
        "rf_mse = mean_squared_error(y_val, y_pred_rf)\n",
        "rf_r2 = r2_score(y_val, y_pred_rf)\n",
        "\n",
        "print(\"🌲 Random Forest Regressor Results:\")\n",
        "print(\"MSE:\", rf_mse)\n",
        "print(\"R² Score:\", rf_r2)\n",
        "\n",
        "# ------------------------------\n",
        "# ⚡ XGBoost Regressor\n",
        "# ------------------------------\n",
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_val_scaled)\n",
        "\n",
        "xgb_mse = mean_squared_error(y_val, y_pred_xgb)\n",
        "xgb_r2 = r2_score(y_val, y_pred_xgb)\n",
        "\n",
        "print(\"\\n⚡ XGBoost Regressor Results:\")\n",
        "print(\"MSE:\", xgb_mse)\n",
        "print(\"R² Score:\", xgb_r2)\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Yrjlb8j9SCry"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from lightgbm import LGBMRegressor\n",
        "\n",
        "lgb_model = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "lgb_model.fit(X_train_scaled, y_train)\n",
        "y_pred_lgb = lgb_model.predict(X_val_scaled)\n",
        "\n",
        "lgb_mse = mean_squared_error(y_val, y_pred_lgb)\n",
        "lgb_r2 = r2_score(y_val, y_pred_lgb)\n",
        "\n",
        "print(\"🌿 LightGBM Regressor Results:\")\n",
        "print(\"MSE:\", lgb_mse)\n",
        "print(\"R² Score:\", lgb_r2)\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kt4gOi5-SCrz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Define your base model\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'subsample': [0.7, 1.0],\n",
        "    'colsample_bytree': [0.7, 1.0],\n",
        "    'reg_alpha': [0, 0.1],\n",
        "    'reg_lambda': [1, 10]\n",
        "}\n",
        "\n",
        "# Setup the GridSearch\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='r2',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit to the data\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Best results\n",
        "print(\"Best Parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"Best R² Score from CV:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on full data\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred_best_xgb = best_xgb.predict(X)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse_best = mean_squared_error(y, y_pred_best_xgb)\n",
        "r2_best = r2_score(y, y_pred_best_xgb)\n",
        "\n",
        "print(\"\\n🎯 Tuned XGBoost Regressor Results:\")\n",
        "print(\"MSE:\", mse_best)\n",
        "print(\"R² Score:\", r2_best)\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "N7-YV7lNSCrz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from xgboost import XGBRegressor\n",
        "\n",
        "# Train the tuned XGBoost model again\n",
        "tuned_xgb = XGBRegressor(\n",
        "    colsample_bytree=0.7,\n",
        "    learning_rate=0.2,\n",
        "    max_depth=7,\n",
        "    n_estimators=200,\n",
        "    reg_alpha=0,\n",
        "    reg_lambda=1,\n",
        "    subsample=0.7,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model on the training data\n",
        "tuned_xgb.fit(X, y)\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5CcR3QDNSCrz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# 1. Load Test Data\n",
        "import pandas as pd\n",
        "pjtst = pd.read_csv(\"//kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv\")\n",
        "\n",
        "# 2. Preprocess Test Data — same steps as training\n",
        "# (Use same encoders, scalers if any, and handle missing values)\n",
        "\n",
        "# Example: Simple mode/mean imputation (update with exact steps you used)\n",
        "pjtst['trafficSource.isTrueDirect'].fillna(True, inplace=True)\n",
        "pjtst['trafficSource.keyword'].fillna('(not provided)', inplace=True)\n",
        "pjtst['trafficSource.referralPath'].fillna('/', inplace=True)\n",
        "pjtst['pageViews'].fillna(pjtst['pageViews'].median(), inplace=True)\n",
        "pjtst['totals.bounces'].fillna(pjtst['totals.bounces'].median(), inplace=True)\n",
        "pjtst['new_visits'].fillna(pjtst['new_visits'].median(), inplace=True)\n",
        "\n",
        "# 3. Select the same features used in training\n",
        "X_test = pjtst[X.columns]  # same columns used to train XGBoost\n",
        "\n",
        "# 4. Predict using final model\n",
        "final_predictions = tuned_xgb.predict(X_test)\n",
        "\n",
        "# 5. Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'sessionId': pjtst['sessionId'],    # Or any ID column required\n",
        "    'purchaseValue': final_predictions  # This is the predicted target\n",
        "})\n",
        "\n",
        "# Create the submission DataFrame with correct column names\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': range(len(pjtst['userId'])),                # Rename userId to id\n",
        "    'purchaseValue': final_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV in proper format\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"✅ Submission file created: submission.csv\")\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "EeGvqz7ESCrz"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}